{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import realpath\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import join as join\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4_input (InputLayer)  [(None, 256, 256, 3)]    0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 256, 256, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 256, 256, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 256, 256, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 256, 256, 32)     128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 128, 128, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 128, 128, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 128, 128, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 128, 128, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 128, 128, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 64, 64, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 262144)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               67109120  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,176,998\n",
      "Trainable params: 67,176,614\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Sequential()\n",
    "feature_extractor.add(Conv2D(32, 3, activation = activation, padding = 'same', input_shape = (SIZE, SIZE, 3)))\n",
    "feature_extractor.add(BatchNormalization())\n",
    "\n",
    "feature_extractor.add(Conv2D(32, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "feature_extractor.add(BatchNormalization())\n",
    "feature_extractor.add(MaxPooling2D())\n",
    "\n",
    "feature_extractor.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "feature_extractor.add(BatchNormalization())\n",
    "\n",
    "feature_extractor.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\n",
    "feature_extractor.add(BatchNormalization())\n",
    "feature_extractor.add(MaxPooling2D())\n",
    "\n",
    "feature_extractor.add(Flatten())\n",
    "\n",
    "\n",
    "x = feature_extractor.output  \n",
    "x = Dense(256, activation = activation, kernel_initializer = 'he_uniform')(x)\n",
    "prediction_layer = Dense(6, activation = 'sigmoid')(x)\n",
    "\n",
    "cnn_model = Model(inputs=feature_extractor.input, outputs=prediction_layer)\n",
    "cnn_model.compile(optimizer='rmsprop',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "print(cnn_model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interno = pd.read_csv('./df_interno.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interno.replace({\"CLASSE\":[6, 7]}, 5, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASSE</th>\n",
       "      <th>VIGOR</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>196598</th>\n",
       "      <th>196599</th>\n",
       "      <th>196600</th>\n",
       "      <th>196601</th>\n",
       "      <th>196602</th>\n",
       "      <th>196603</th>\n",
       "      <th>196604</th>\n",
       "      <th>196605</th>\n",
       "      <th>196606</th>\n",
       "      <th>196607</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.290196</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.286275</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.27451</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 196610 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CLASSE  VIGOR         0         1         2         3         4  \\\n",
       "0         4      1  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1         5      1  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2         5      1  0.392157  0.380392  0.439216  0.415686  0.403922   \n",
       "3         3      0  0.000000  0.000000  0.000000  0.352941  0.301961   \n",
       "4         4      1  0.000000  0.000000  0.000000  0.294118  0.247059   \n",
       "..      ...    ...       ...       ...       ...       ...       ...   \n",
       "445       4      1  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "446       4      1  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "447       2      0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "448       5      1  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "449       2      0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            5         6         7  ...  196598  196599  196600  196601  \\\n",
       "0    0.000000  0.000000  0.000000  ...     0.0     0.0     0.0     0.0   \n",
       "1    0.000000  0.000000  0.000000  ...     0.0     0.0     0.0     0.0   \n",
       "2    0.466667  0.403922  0.400000  ...     0.0     0.0     0.0     0.0   \n",
       "3    0.364706  0.290196  0.262745  ...     0.0     0.0     0.0     0.0   \n",
       "4    0.305882  0.286275  0.247059  ...     0.0     0.0     0.0     0.0   \n",
       "..        ...       ...       ...  ...     ...     ...     ...     ...   \n",
       "445  0.000000  0.000000  0.000000  ...     0.0     0.0     0.0     0.0   \n",
       "446  0.000000  0.000000  0.000000  ...     0.0     0.0     0.0     0.0   \n",
       "447  0.000000  0.000000  0.000000  ...     0.0     0.0     0.0     0.0   \n",
       "448  0.000000  0.000000  0.000000  ...     0.0     0.0     0.0     0.0   \n",
       "449  0.000000  0.000000  0.000000  ...     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       196602    196603    196604    196605   196606    196607  \n",
       "0    0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "4    0.298039  0.258824  0.321569  0.301961  0.27451  0.333333  \n",
       "..        ...       ...       ...       ...      ...       ...  \n",
       "445  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "446  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "447  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "448  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "449  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "\n",
       "[450 rows x 196610 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vigor = df_interno['VIGOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = df_interno['CLASSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 3, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classe.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interno.drop(['CLASSE', 'VIGOR'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 196608)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interno.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interno = df_interno.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interno = np.reshape(df_interno, (450, 256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = classe.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  #Random Forest algorithm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_interno, classe, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11/11 [==============================] - 48s 4s/step - loss: 3.2492 - accuracy: 0.5223 - val_loss: 2.5652 - val_accuracy: 0.5487\n",
      "Epoch 2/1000\n",
      "11/11 [==============================] - 47s 4s/step - loss: 1.8815 - accuracy: 0.6024 - val_loss: 1.7382 - val_accuracy: 0.5487\n",
      "Epoch 3/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 1.4362 - accuracy: 0.6053 - val_loss: 1.4383 - val_accuracy: 0.5487\n",
      "Epoch 4/1000\n",
      "11/11 [==============================] - 47s 4s/step - loss: 1.2422 - accuracy: 0.6053 - val_loss: 1.2696 - val_accuracy: 0.5487\n",
      "Epoch 5/1000\n",
      "11/11 [==============================] - 46s 4s/step - loss: 1.1319 - accuracy: 0.6113 - val_loss: 1.1870 - val_accuracy: 0.5487\n",
      "Epoch 6/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0944 - accuracy: 0.6202 - val_loss: 1.1432 - val_accuracy: 0.5487\n",
      "Epoch 7/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0791 - accuracy: 0.6024 - val_loss: 1.2789 - val_accuracy: 0.2566\n",
      "Epoch 8/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 1.0935 - accuracy: 0.6202 - val_loss: 1.4011 - val_accuracy: 0.2566\n",
      "Epoch 9/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0816 - accuracy: 0.6172 - val_loss: 1.4033 - val_accuracy: 0.2566\n",
      "Epoch 10/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0938 - accuracy: 0.6202 - val_loss: 1.2354 - val_accuracy: 0.2566\n",
      "Epoch 11/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0890 - accuracy: 0.6113 - val_loss: 1.2030 - val_accuracy: 0.4336\n",
      "Epoch 12/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0742 - accuracy: 0.6202 - val_loss: 1.1547 - val_accuracy: 0.5487\n",
      "Epoch 13/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0898 - accuracy: 0.6261 - val_loss: 3.9842 - val_accuracy: 0.1858\n",
      "Epoch 14/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 1.0716 - accuracy: 0.6261 - val_loss: 4.4194 - val_accuracy: 0.1858\n",
      "Epoch 15/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0837 - accuracy: 0.6320 - val_loss: 9.4498 - val_accuracy: 0.1327\n",
      "Epoch 16/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0895 - accuracy: 0.6261 - val_loss: 9.6607 - val_accuracy: 0.1327\n",
      "Epoch 17/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0762 - accuracy: 0.6261 - val_loss: 9.6482 - val_accuracy: 0.1327\n",
      "Epoch 18/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0618 - accuracy: 0.6320 - val_loss: 9.6967 - val_accuracy: 0.1327\n",
      "Epoch 19/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0720 - accuracy: 0.6172 - val_loss: 9.7624 - val_accuracy: 0.1327\n",
      "Epoch 20/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0846 - accuracy: 0.6083 - val_loss: 9.7409 - val_accuracy: 0.1327\n",
      "Epoch 21/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0579 - accuracy: 0.6231 - val_loss: 9.8903 - val_accuracy: 0.1327\n",
      "Epoch 22/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0653 - accuracy: 0.6172 - val_loss: 9.8494 - val_accuracy: 0.1327\n",
      "Epoch 23/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0695 - accuracy: 0.6261 - val_loss: 9.9309 - val_accuracy: 0.1327\n",
      "Epoch 24/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0657 - accuracy: 0.6350 - val_loss: 9.4161 - val_accuracy: 0.1327\n",
      "Epoch 25/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0732 - accuracy: 0.6231 - val_loss: 9.3566 - val_accuracy: 0.1327\n",
      "Epoch 26/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0658 - accuracy: 0.6231 - val_loss: 9.0581 - val_accuracy: 0.1327\n",
      "Epoch 27/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0554 - accuracy: 0.6320 - val_loss: 9.3023 - val_accuracy: 0.1327\n",
      "Epoch 28/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0497 - accuracy: 0.6350 - val_loss: 9.4469 - val_accuracy: 0.1327\n",
      "Epoch 29/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0644 - accuracy: 0.6350 - val_loss: 9.3437 - val_accuracy: 0.1327\n",
      "Epoch 30/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0519 - accuracy: 0.6409 - val_loss: 9.2343 - val_accuracy: 0.1327\n",
      "Epoch 31/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0505 - accuracy: 0.6231 - val_loss: 9.5117 - val_accuracy: 0.1327\n",
      "Epoch 32/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0505 - accuracy: 0.6350 - val_loss: 9.4087 - val_accuracy: 0.1327\n",
      "Epoch 33/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0421 - accuracy: 0.6350 - val_loss: 9.7552 - val_accuracy: 0.1327\n",
      "Epoch 34/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0334 - accuracy: 0.6439 - val_loss: 9.5509 - val_accuracy: 0.1327\n",
      "Epoch 35/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0369 - accuracy: 0.6439 - val_loss: 9.8562 - val_accuracy: 0.1327\n",
      "Epoch 36/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0487 - accuracy: 0.6350 - val_loss: 10.0722 - val_accuracy: 0.1327\n",
      "Epoch 37/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0470 - accuracy: 0.6231 - val_loss: 10.0709 - val_accuracy: 0.1327\n",
      "Epoch 38/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0330 - accuracy: 0.6320 - val_loss: 10.0222 - val_accuracy: 0.1327\n",
      "Epoch 39/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0371 - accuracy: 0.6409 - val_loss: 9.8912 - val_accuracy: 0.1327\n",
      "Epoch 40/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0349 - accuracy: 0.6469 - val_loss: 9.8503 - val_accuracy: 0.1327\n",
      "Epoch 41/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0243 - accuracy: 0.6499 - val_loss: 9.6855 - val_accuracy: 0.1327\n",
      "Epoch 42/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0205 - accuracy: 0.6409 - val_loss: 6.5295 - val_accuracy: 0.1239\n",
      "Epoch 43/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0296 - accuracy: 0.6409 - val_loss: 6.2494 - val_accuracy: 0.0973\n",
      "Epoch 44/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0151 - accuracy: 0.6469 - val_loss: 5.5242 - val_accuracy: 0.1681\n",
      "Epoch 45/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0266 - accuracy: 0.6350 - val_loss: 1.5215 - val_accuracy: 0.4690\n",
      "Epoch 46/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0256 - accuracy: 0.6380 - val_loss: 1.8060 - val_accuracy: 0.3540\n",
      "Epoch 47/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0272 - accuracy: 0.6439 - val_loss: 9.8108 - val_accuracy: 0.1327\n",
      "Epoch 48/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0287 - accuracy: 0.6439 - val_loss: 7.9270 - val_accuracy: 0.1504\n",
      "Epoch 49/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0407 - accuracy: 0.6409 - val_loss: 1.1673 - val_accuracy: 0.5221\n",
      "Epoch 50/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0260 - accuracy: 0.6409 - val_loss: 1.1888 - val_accuracy: 0.5664\n",
      "Epoch 51/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0204 - accuracy: 0.6439 - val_loss: 1.1505 - val_accuracy: 0.5487\n",
      "Epoch 52/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0175 - accuracy: 0.6499 - val_loss: 1.1686 - val_accuracy: 0.5664\n",
      "Epoch 53/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0531 - accuracy: 0.6439 - val_loss: 6.7900 - val_accuracy: 0.1327\n",
      "Epoch 54/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0407 - accuracy: 0.6380 - val_loss: 1.4556 - val_accuracy: 0.4248\n",
      "Epoch 55/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0114 - accuracy: 0.6499 - val_loss: 1.1863 - val_accuracy: 0.5575\n",
      "Epoch 56/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0157 - accuracy: 0.6439 - val_loss: 1.1505 - val_accuracy: 0.5664\n",
      "Epoch 57/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0197 - accuracy: 0.6469 - val_loss: 1.2007 - val_accuracy: 0.5664\n",
      "Epoch 58/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0071 - accuracy: 0.6499 - val_loss: 1.1708 - val_accuracy: 0.5664\n",
      "Epoch 59/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0000 - accuracy: 0.6617 - val_loss: 1.1519 - val_accuracy: 0.5752\n",
      "Epoch 60/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0053 - accuracy: 0.6558 - val_loss: 1.1588 - val_accuracy: 0.5841\n",
      "Epoch 61/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0235 - accuracy: 0.6320 - val_loss: 1.4500 - val_accuracy: 0.3186\n",
      "Epoch 62/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0073 - accuracy: 0.6469 - val_loss: 1.2192 - val_accuracy: 0.5398\n",
      "Epoch 63/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0064 - accuracy: 0.6499 - val_loss: 1.1567 - val_accuracy: 0.5929\n",
      "Epoch 64/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9936 - accuracy: 0.6617 - val_loss: 1.2071 - val_accuracy: 0.5929\n",
      "Epoch 65/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0132 - accuracy: 0.6558 - val_loss: 1.2058 - val_accuracy: 0.5929\n",
      "Epoch 66/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9815 - accuracy: 0.6647 - val_loss: 1.1546 - val_accuracy: 0.5752\n",
      "Epoch 67/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9998 - accuracy: 0.6499 - val_loss: 1.2214 - val_accuracy: 0.5221\n",
      "Epoch 68/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0068 - accuracy: 0.6558 - val_loss: 4.0408 - val_accuracy: 0.1327\n",
      "Epoch 69/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0069 - accuracy: 0.6469 - val_loss: 1.2671 - val_accuracy: 0.4867\n",
      "Epoch 70/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0261 - accuracy: 0.6528 - val_loss: 1.8591 - val_accuracy: 0.2655\n",
      "Epoch 71/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0248 - accuracy: 0.6439 - val_loss: 2.7433 - val_accuracy: 0.1416\n",
      "Epoch 72/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0144 - accuracy: 0.6380 - val_loss: 1.2341 - val_accuracy: 0.4867\n",
      "Epoch 73/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.0092 - accuracy: 0.6350 - val_loss: 1.2450 - val_accuracy: 0.4602\n",
      "Epoch 74/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0044 - accuracy: 0.6558 - val_loss: 1.2225 - val_accuracy: 0.5575\n",
      "Epoch 75/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 1.0156 - accuracy: 0.6469 - val_loss: 1.1788 - val_accuracy: 0.5398\n",
      "Epoch 76/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9934 - accuracy: 0.6588 - val_loss: 1.1992 - val_accuracy: 0.5752\n",
      "Epoch 77/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9666 - accuracy: 0.6588 - val_loss: 1.2198 - val_accuracy: 0.5841\n",
      "Epoch 78/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9731 - accuracy: 0.6617 - val_loss: 1.1662 - val_accuracy: 0.5664\n",
      "Epoch 79/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9646 - accuracy: 0.6617 - val_loss: 1.1772 - val_accuracy: 0.5487\n",
      "Epoch 80/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9780 - accuracy: 0.6617 - val_loss: 1.2121 - val_accuracy: 0.5841\n",
      "Epoch 81/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9681 - accuracy: 0.6706 - val_loss: 1.2189 - val_accuracy: 0.5752\n",
      "Epoch 82/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9694 - accuracy: 0.6617 - val_loss: 1.2235 - val_accuracy: 0.5841\n",
      "Epoch 83/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9578 - accuracy: 0.6528 - val_loss: 1.1899 - val_accuracy: 0.5752\n",
      "Epoch 84/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9563 - accuracy: 0.6855 - val_loss: 1.2737 - val_accuracy: 0.5841\n",
      "Epoch 85/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9798 - accuracy: 0.6617 - val_loss: 1.2370 - val_accuracy: 0.5929\n",
      "Epoch 86/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9613 - accuracy: 0.6617 - val_loss: 1.2449 - val_accuracy: 0.5752\n",
      "Epoch 87/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9742 - accuracy: 0.6677 - val_loss: 1.2274 - val_accuracy: 0.5398\n",
      "Epoch 88/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9731 - accuracy: 0.6617 - val_loss: 1.5584 - val_accuracy: 0.3274\n",
      "Epoch 89/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9692 - accuracy: 0.6677 - val_loss: 1.2022 - val_accuracy: 0.5752\n",
      "Epoch 90/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9604 - accuracy: 0.6617 - val_loss: 1.1565 - val_accuracy: 0.5752\n",
      "Epoch 91/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9759 - accuracy: 0.6647 - val_loss: 1.2029 - val_accuracy: 0.5398\n",
      "Epoch 92/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9814 - accuracy: 0.6617 - val_loss: 1.1881 - val_accuracy: 0.5752\n",
      "Epoch 93/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9869 - accuracy: 0.6528 - val_loss: 1.1514 - val_accuracy: 0.5841\n",
      "Epoch 94/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9766 - accuracy: 0.6677 - val_loss: 1.1629 - val_accuracy: 0.5929\n",
      "Epoch 95/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9697 - accuracy: 0.6706 - val_loss: 1.2094 - val_accuracy: 0.5841\n",
      "Epoch 96/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9760 - accuracy: 0.6677 - val_loss: 1.2000 - val_accuracy: 0.5752\n",
      "Epoch 97/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9681 - accuracy: 0.6588 - val_loss: 1.1932 - val_accuracy: 0.5664\n",
      "Epoch 98/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9658 - accuracy: 0.6528 - val_loss: 1.1748 - val_accuracy: 0.5575\n",
      "Epoch 99/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9620 - accuracy: 0.6677 - val_loss: 1.1523 - val_accuracy: 0.5487\n",
      "Epoch 100/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9487 - accuracy: 0.6617 - val_loss: 1.1961 - val_accuracy: 0.5044\n",
      "Epoch 101/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9613 - accuracy: 0.6706 - val_loss: 1.2055 - val_accuracy: 0.5044\n",
      "Epoch 102/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9632 - accuracy: 0.6617 - val_loss: 1.1569 - val_accuracy: 0.5575\n",
      "Epoch 103/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9599 - accuracy: 0.6647 - val_loss: 1.1997 - val_accuracy: 0.5664\n",
      "Epoch 104/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9501 - accuracy: 0.6736 - val_loss: 1.1788 - val_accuracy: 0.5487\n",
      "Epoch 105/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.9531 - accuracy: 0.6677 - val_loss: 1.2187 - val_accuracy: 0.5575\n",
      "Epoch 106/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.9786 - accuracy: 0.6647 - val_loss: 1.2093 - val_accuracy: 0.5398\n",
      "Epoch 107/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.9534 - accuracy: 0.6706 - val_loss: 1.6820 - val_accuracy: 0.3717\n",
      "Epoch 108/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.9951 - accuracy: 0.6469 - val_loss: 1.2786 - val_accuracy: 0.4513\n",
      "Epoch 109/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.9726 - accuracy: 0.6499 - val_loss: 1.1998 - val_accuracy: 0.5575\n",
      "Epoch 110/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.9727 - accuracy: 0.6677 - val_loss: 1.2472 - val_accuracy: 0.5664\n",
      "Epoch 111/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.9493 - accuracy: 0.6677 - val_loss: 1.2169 - val_accuracy: 0.5752\n",
      "Epoch 112/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.9515 - accuracy: 0.6736 - val_loss: 1.2112 - val_accuracy: 0.5487\n",
      "Epoch 113/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9503 - accuracy: 0.6617 - val_loss: 1.2580 - val_accuracy: 0.5575\n",
      "Epoch 114/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9518 - accuracy: 0.6677 - val_loss: 1.2336 - val_accuracy: 0.5664\n",
      "Epoch 115/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9453 - accuracy: 0.6766 - val_loss: 1.2120 - val_accuracy: 0.5575\n",
      "Epoch 116/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9450 - accuracy: 0.6795 - val_loss: 1.2168 - val_accuracy: 0.5752\n",
      "Epoch 117/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9431 - accuracy: 0.6736 - val_loss: 1.2342 - val_accuracy: 0.5664\n",
      "Epoch 118/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9410 - accuracy: 0.6706 - val_loss: 1.2231 - val_accuracy: 0.5487\n",
      "Epoch 119/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9545 - accuracy: 0.6736 - val_loss: 1.1842 - val_accuracy: 0.5398\n",
      "Epoch 120/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9454 - accuracy: 0.6825 - val_loss: 1.3829 - val_accuracy: 0.4602\n",
      "Epoch 121/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9408 - accuracy: 0.6766 - val_loss: 1.2552 - val_accuracy: 0.4867\n",
      "Epoch 122/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9220 - accuracy: 0.6884 - val_loss: 1.2821 - val_accuracy: 0.5221\n",
      "Epoch 123/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9299 - accuracy: 0.6795 - val_loss: 1.2429 - val_accuracy: 0.5841\n",
      "Epoch 124/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9078 - accuracy: 0.6884 - val_loss: 1.2546 - val_accuracy: 0.5664\n",
      "Epoch 125/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9171 - accuracy: 0.6944 - val_loss: 1.2481 - val_accuracy: 0.5664\n",
      "Epoch 126/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9233 - accuracy: 0.6884 - val_loss: 1.1850 - val_accuracy: 0.5752\n",
      "Epoch 127/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9095 - accuracy: 0.6855 - val_loss: 1.1459 - val_accuracy: 0.5752\n",
      "Epoch 128/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9193 - accuracy: 0.6766 - val_loss: 1.2001 - val_accuracy: 0.5664\n",
      "Epoch 129/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9274 - accuracy: 0.6795 - val_loss: 1.1727 - val_accuracy: 0.5664\n",
      "Epoch 130/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9237 - accuracy: 0.6825 - val_loss: 1.1975 - val_accuracy: 0.5929\n",
      "Epoch 131/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9242 - accuracy: 0.6766 - val_loss: 1.2133 - val_accuracy: 0.5841\n",
      "Epoch 132/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9213 - accuracy: 0.6825 - val_loss: 1.2092 - val_accuracy: 0.5929\n",
      "Epoch 133/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9179 - accuracy: 0.6914 - val_loss: 1.1723 - val_accuracy: 0.5929\n",
      "Epoch 134/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9439 - accuracy: 0.6766 - val_loss: 1.2066 - val_accuracy: 0.5929\n",
      "Epoch 135/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9179 - accuracy: 0.6855 - val_loss: 1.2263 - val_accuracy: 0.5752\n",
      "Epoch 136/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9171 - accuracy: 0.6855 - val_loss: 1.1686 - val_accuracy: 0.5487\n",
      "Epoch 137/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9295 - accuracy: 0.6825 - val_loss: 1.2172 - val_accuracy: 0.5575\n",
      "Epoch 138/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9318 - accuracy: 0.6766 - val_loss: 1.1874 - val_accuracy: 0.5929\n",
      "Epoch 139/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9201 - accuracy: 0.6736 - val_loss: 1.1905 - val_accuracy: 0.5841\n",
      "Epoch 140/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9215 - accuracy: 0.6795 - val_loss: 1.1990 - val_accuracy: 0.5487\n",
      "Epoch 141/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9176 - accuracy: 0.6736 - val_loss: 1.2631 - val_accuracy: 0.5841\n",
      "Epoch 142/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9118 - accuracy: 0.6944 - val_loss: 1.3244 - val_accuracy: 0.4690\n",
      "Epoch 143/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9376 - accuracy: 0.6706 - val_loss: 1.2761 - val_accuracy: 0.5752\n",
      "Epoch 144/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9177 - accuracy: 0.6766 - val_loss: 1.2326 - val_accuracy: 0.5487\n",
      "Epoch 145/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9190 - accuracy: 0.6855 - val_loss: 1.2323 - val_accuracy: 0.5664\n",
      "Epoch 146/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9189 - accuracy: 0.6944 - val_loss: 1.2222 - val_accuracy: 0.5752\n",
      "Epoch 147/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9031 - accuracy: 0.6914 - val_loss: 1.2274 - val_accuracy: 0.5398\n",
      "Epoch 148/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9049 - accuracy: 0.6825 - val_loss: 1.2477 - val_accuracy: 0.5398\n",
      "Epoch 149/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9152 - accuracy: 0.6855 - val_loss: 1.2230 - val_accuracy: 0.5398\n",
      "Epoch 150/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8991 - accuracy: 0.6884 - val_loss: 1.1956 - val_accuracy: 0.5752\n",
      "Epoch 151/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9097 - accuracy: 0.6766 - val_loss: 1.2541 - val_accuracy: 0.5664\n",
      "Epoch 152/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9033 - accuracy: 0.6825 - val_loss: 1.2485 - val_accuracy: 0.5575\n",
      "Epoch 153/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8962 - accuracy: 0.6973 - val_loss: 1.2285 - val_accuracy: 0.5487\n",
      "Epoch 154/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9067 - accuracy: 0.6855 - val_loss: 1.2102 - val_accuracy: 0.5575\n",
      "Epoch 155/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8923 - accuracy: 0.7033 - val_loss: 1.2498 - val_accuracy: 0.5487\n",
      "Epoch 156/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8937 - accuracy: 0.6973 - val_loss: 1.2226 - val_accuracy: 0.5575\n",
      "Epoch 157/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8884 - accuracy: 0.6884 - val_loss: 1.2295 - val_accuracy: 0.5310\n",
      "Epoch 158/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9139 - accuracy: 0.6855 - val_loss: 1.2092 - val_accuracy: 0.5752\n",
      "Epoch 159/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9109 - accuracy: 0.6914 - val_loss: 1.2047 - val_accuracy: 0.5752\n",
      "Epoch 160/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9024 - accuracy: 0.6884 - val_loss: 1.2066 - val_accuracy: 0.5664\n",
      "Epoch 161/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9052 - accuracy: 0.6944 - val_loss: 1.2010 - val_accuracy: 0.5575\n",
      "Epoch 162/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9106 - accuracy: 0.6825 - val_loss: 1.2429 - val_accuracy: 0.5664\n",
      "Epoch 163/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8980 - accuracy: 0.6914 - val_loss: 1.1811 - val_accuracy: 0.5929\n",
      "Epoch 164/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.9060 - accuracy: 0.6944 - val_loss: 1.2370 - val_accuracy: 0.5664\n",
      "Epoch 165/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8955 - accuracy: 0.6973 - val_loss: 1.2900 - val_accuracy: 0.5221\n",
      "Epoch 166/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9152 - accuracy: 0.6914 - val_loss: 2.0374 - val_accuracy: 0.2035\n",
      "Epoch 167/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9280 - accuracy: 0.6884 - val_loss: 1.4447 - val_accuracy: 0.4071\n",
      "Epoch 168/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9095 - accuracy: 0.6795 - val_loss: 1.3237 - val_accuracy: 0.5221\n",
      "Epoch 169/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9082 - accuracy: 0.6973 - val_loss: 1.2779 - val_accuracy: 0.5752\n",
      "Epoch 170/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9029 - accuracy: 0.6884 - val_loss: 1.2071 - val_accuracy: 0.5752\n",
      "Epoch 171/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8843 - accuracy: 0.6914 - val_loss: 1.1391 - val_accuracy: 0.5929\n",
      "Epoch 172/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.8758 - accuracy: 0.6944 - val_loss: 1.2197 - val_accuracy: 0.5752\n",
      "Epoch 173/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8793 - accuracy: 0.7062 - val_loss: 1.2252 - val_accuracy: 0.5929\n",
      "Epoch 174/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8851 - accuracy: 0.7151 - val_loss: 1.2680 - val_accuracy: 0.5929\n",
      "Epoch 175/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8921 - accuracy: 0.7003 - val_loss: 1.2071 - val_accuracy: 0.5752\n",
      "Epoch 176/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8836 - accuracy: 0.7092 - val_loss: 1.2345 - val_accuracy: 0.5310\n",
      "Epoch 177/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8858 - accuracy: 0.7003 - val_loss: 1.2221 - val_accuracy: 0.5398\n",
      "Epoch 178/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8982 - accuracy: 0.7181 - val_loss: 1.2300 - val_accuracy: 0.5841\n",
      "Epoch 179/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8925 - accuracy: 0.7003 - val_loss: 1.2618 - val_accuracy: 0.5752\n",
      "Epoch 180/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8871 - accuracy: 0.7062 - val_loss: 1.2417 - val_accuracy: 0.5929\n",
      "Epoch 181/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8839 - accuracy: 0.6973 - val_loss: 1.2179 - val_accuracy: 0.5752\n",
      "Epoch 182/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8767 - accuracy: 0.7151 - val_loss: 1.2001 - val_accuracy: 0.5841\n",
      "Epoch 183/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8777 - accuracy: 0.7122 - val_loss: 1.2240 - val_accuracy: 0.5664\n",
      "Epoch 184/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8787 - accuracy: 0.7122 - val_loss: 1.3108 - val_accuracy: 0.5310\n",
      "Epoch 185/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8721 - accuracy: 0.7033 - val_loss: 1.2054 - val_accuracy: 0.5487\n",
      "Epoch 186/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8724 - accuracy: 0.7033 - val_loss: 1.2077 - val_accuracy: 0.5221\n",
      "Epoch 187/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8652 - accuracy: 0.7092 - val_loss: 1.2309 - val_accuracy: 0.5575\n",
      "Epoch 188/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8612 - accuracy: 0.7181 - val_loss: 1.2645 - val_accuracy: 0.5398\n",
      "Epoch 189/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8694 - accuracy: 0.7122 - val_loss: 1.2253 - val_accuracy: 0.5575\n",
      "Epoch 190/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8672 - accuracy: 0.7033 - val_loss: 1.2412 - val_accuracy: 0.5044\n",
      "Epoch 191/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8800 - accuracy: 0.7033 - val_loss: 1.1739 - val_accuracy: 0.5575\n",
      "Epoch 192/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8580 - accuracy: 0.7122 - val_loss: 1.2037 - val_accuracy: 0.5752\n",
      "Epoch 193/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8532 - accuracy: 0.7092 - val_loss: 1.2204 - val_accuracy: 0.5575\n",
      "Epoch 194/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8894 - accuracy: 0.7062 - val_loss: 1.2351 - val_accuracy: 0.5929\n",
      "Epoch 195/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8722 - accuracy: 0.7092 - val_loss: 1.1893 - val_accuracy: 0.5664\n",
      "Epoch 196/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8382 - accuracy: 0.7181 - val_loss: 1.2498 - val_accuracy: 0.5487\n",
      "Epoch 197/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8370 - accuracy: 0.7092 - val_loss: 1.2179 - val_accuracy: 0.5487\n",
      "Epoch 198/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8513 - accuracy: 0.7181 - val_loss: 1.2862 - val_accuracy: 0.5398\n",
      "Epoch 199/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8592 - accuracy: 0.7062 - val_loss: 1.2705 - val_accuracy: 0.5398\n",
      "Epoch 200/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8563 - accuracy: 0.7151 - val_loss: 1.2661 - val_accuracy: 0.5664\n",
      "Epoch 201/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8464 - accuracy: 0.7092 - val_loss: 1.2545 - val_accuracy: 0.5841\n",
      "Epoch 202/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8599 - accuracy: 0.7122 - val_loss: 1.2313 - val_accuracy: 0.5664\n",
      "Epoch 203/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8607 - accuracy: 0.7033 - val_loss: 1.2838 - val_accuracy: 0.5575\n",
      "Epoch 204/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8684 - accuracy: 0.7033 - val_loss: 1.2056 - val_accuracy: 0.5664\n",
      "Epoch 205/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8360 - accuracy: 0.7240 - val_loss: 1.6755 - val_accuracy: 0.4071\n",
      "Epoch 206/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8575 - accuracy: 0.7033 - val_loss: 1.3616 - val_accuracy: 0.4779\n",
      "Epoch 207/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8446 - accuracy: 0.7033 - val_loss: 1.2102 - val_accuracy: 0.5398\n",
      "Epoch 208/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8442 - accuracy: 0.7181 - val_loss: 1.2949 - val_accuracy: 0.5221\n",
      "Epoch 209/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8595 - accuracy: 0.7122 - val_loss: 1.3085 - val_accuracy: 0.5133\n",
      "Epoch 210/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8490 - accuracy: 0.7092 - val_loss: 1.3699 - val_accuracy: 0.5310\n",
      "Epoch 211/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8456 - accuracy: 0.6914 - val_loss: 1.4950 - val_accuracy: 0.4956\n",
      "Epoch 212/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8704 - accuracy: 0.6914 - val_loss: 1.3913 - val_accuracy: 0.5221\n",
      "Epoch 213/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8483 - accuracy: 0.7092 - val_loss: 1.2403 - val_accuracy: 0.5752\n",
      "Epoch 214/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8480 - accuracy: 0.7122 - val_loss: 1.2720 - val_accuracy: 0.5487\n",
      "Epoch 215/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8539 - accuracy: 0.7092 - val_loss: 1.2014 - val_accuracy: 0.5310\n",
      "Epoch 216/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8461 - accuracy: 0.7211 - val_loss: 1.3727 - val_accuracy: 0.5044\n",
      "Epoch 217/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8613 - accuracy: 0.7092 - val_loss: 1.3427 - val_accuracy: 0.5133\n",
      "Epoch 218/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8479 - accuracy: 0.7211 - val_loss: 1.2018 - val_accuracy: 0.5664\n",
      "Epoch 219/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8476 - accuracy: 0.7151 - val_loss: 1.2553 - val_accuracy: 0.5752\n",
      "Epoch 220/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8426 - accuracy: 0.7211 - val_loss: 1.2242 - val_accuracy: 0.5752\n",
      "Epoch 221/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8404 - accuracy: 0.7151 - val_loss: 1.2534 - val_accuracy: 0.5664\n",
      "Epoch 222/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8367 - accuracy: 0.7122 - val_loss: 1.3520 - val_accuracy: 0.4690\n",
      "Epoch 223/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8536 - accuracy: 0.7092 - val_loss: 1.2374 - val_accuracy: 0.5487\n",
      "Epoch 224/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8584 - accuracy: 0.7092 - val_loss: 1.2441 - val_accuracy: 0.5575\n",
      "Epoch 225/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8413 - accuracy: 0.7211 - val_loss: 1.2530 - val_accuracy: 0.5841\n",
      "Epoch 226/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8543 - accuracy: 0.7033 - val_loss: 1.2636 - val_accuracy: 0.5575\n",
      "Epoch 227/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8441 - accuracy: 0.7181 - val_loss: 1.2374 - val_accuracy: 0.5575\n",
      "Epoch 228/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8262 - accuracy: 0.7211 - val_loss: 1.2434 - val_accuracy: 0.5929\n",
      "Epoch 229/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8503 - accuracy: 0.7181 - val_loss: 1.2302 - val_accuracy: 0.5487\n",
      "Epoch 230/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8430 - accuracy: 0.7181 - val_loss: 1.2258 - val_accuracy: 0.5398\n",
      "Epoch 231/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8323 - accuracy: 0.7211 - val_loss: 1.2445 - val_accuracy: 0.5487\n",
      "Epoch 232/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8343 - accuracy: 0.7151 - val_loss: 1.2492 - val_accuracy: 0.5221\n",
      "Epoch 233/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 2.7017 - accuracy: 0.6350 - val_loss: 1.9203 - val_accuracy: 0.3540\n",
      "Epoch 234/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8624 - accuracy: 0.7151 - val_loss: 1.3740 - val_accuracy: 0.5133\n",
      "Epoch 235/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8417 - accuracy: 0.7151 - val_loss: 1.2328 - val_accuracy: 0.5398\n",
      "Epoch 236/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8409 - accuracy: 0.7122 - val_loss: 1.2829 - val_accuracy: 0.5575\n",
      "Epoch 237/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8283 - accuracy: 0.7329 - val_loss: 1.2973 - val_accuracy: 0.5398\n",
      "Epoch 238/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8323 - accuracy: 0.7181 - val_loss: 1.2941 - val_accuracy: 0.5487\n",
      "Epoch 239/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8287 - accuracy: 0.7181 - val_loss: 1.3983 - val_accuracy: 0.5044\n",
      "Epoch 240/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8287 - accuracy: 0.7181 - val_loss: 1.3208 - val_accuracy: 0.5133\n",
      "Epoch 241/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8418 - accuracy: 0.7211 - val_loss: 1.3021 - val_accuracy: 0.5221\n",
      "Epoch 242/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8329 - accuracy: 0.7211 - val_loss: 1.3522 - val_accuracy: 0.5044\n",
      "Epoch 243/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8155 - accuracy: 0.7359 - val_loss: 1.3776 - val_accuracy: 0.5487\n",
      "Epoch 244/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8196 - accuracy: 0.7359 - val_loss: 1.3832 - val_accuracy: 0.5310\n",
      "Epoch 245/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8011 - accuracy: 0.7329 - val_loss: 1.4728 - val_accuracy: 0.5044\n",
      "Epoch 246/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8076 - accuracy: 0.7240 - val_loss: 1.3830 - val_accuracy: 0.5310\n",
      "Epoch 247/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8088 - accuracy: 0.7300 - val_loss: 1.3039 - val_accuracy: 0.5044\n",
      "Epoch 248/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8206 - accuracy: 0.7092 - val_loss: 7.9762 - val_accuracy: 0.1681\n",
      "Epoch 249/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8144 - accuracy: 0.7300 - val_loss: 1.6670 - val_accuracy: 0.4690\n",
      "Epoch 250/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8120 - accuracy: 0.7270 - val_loss: 1.3473 - val_accuracy: 0.4956\n",
      "Epoch 251/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7894 - accuracy: 0.7418 - val_loss: 1.2976 - val_accuracy: 0.5310\n",
      "Epoch 252/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7883 - accuracy: 0.7329 - val_loss: 1.3454 - val_accuracy: 0.4956\n",
      "Epoch 253/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8058 - accuracy: 0.7359 - val_loss: 1.3244 - val_accuracy: 0.5133\n",
      "Epoch 254/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7760 - accuracy: 0.7240 - val_loss: 1.3213 - val_accuracy: 0.5044\n",
      "Epoch 255/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7838 - accuracy: 0.7359 - val_loss: 1.3244 - val_accuracy: 0.4779\n",
      "Epoch 256/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7817 - accuracy: 0.7211 - val_loss: 1.2918 - val_accuracy: 0.5133\n",
      "Epoch 257/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7808 - accuracy: 0.7389 - val_loss: 1.2888 - val_accuracy: 0.5221\n",
      "Epoch 258/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7996 - accuracy: 0.7359 - val_loss: 1.2879 - val_accuracy: 0.5310\n",
      "Epoch 259/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7952 - accuracy: 0.7448 - val_loss: 1.2901 - val_accuracy: 0.5398\n",
      "Epoch 260/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7946 - accuracy: 0.7389 - val_loss: 1.2735 - val_accuracy: 0.5310\n",
      "Epoch 261/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7828 - accuracy: 0.7329 - val_loss: 1.2388 - val_accuracy: 0.5221\n",
      "Epoch 262/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7976 - accuracy: 0.7329 - val_loss: 1.2581 - val_accuracy: 0.5310\n",
      "Epoch 263/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7998 - accuracy: 0.7418 - val_loss: 1.2537 - val_accuracy: 0.5310\n",
      "Epoch 264/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8172 - accuracy: 0.7300 - val_loss: 1.4856 - val_accuracy: 0.4956\n",
      "Epoch 265/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 11.5754 - accuracy: 0.3561 - val_loss: 14.1872 - val_accuracy: 0.1681\n",
      "Epoch 266/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 14.9657 - accuracy: 0.1395 - val_loss: 1.5557 - val_accuracy: 0.5044\n",
      "Epoch 267/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 1.6885 - accuracy: 0.6469 - val_loss: 1.3203 - val_accuracy: 0.5221\n",
      "Epoch 268/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8442 - accuracy: 0.7151 - val_loss: 1.3202 - val_accuracy: 0.5310\n",
      "Epoch 269/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8415 - accuracy: 0.7240 - val_loss: 1.2829 - val_accuracy: 0.5310\n",
      "Epoch 270/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8279 - accuracy: 0.7181 - val_loss: 1.2096 - val_accuracy: 0.5575\n",
      "Epoch 271/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.9018 - accuracy: 0.7211 - val_loss: 1.2639 - val_accuracy: 0.5398\n",
      "Epoch 272/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8615 - accuracy: 0.7389 - val_loss: 1.2229 - val_accuracy: 0.5664\n",
      "Epoch 273/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8516 - accuracy: 0.7211 - val_loss: 1.2475 - val_accuracy: 0.5310\n",
      "Epoch 274/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8521 - accuracy: 0.7181 - val_loss: 1.1902 - val_accuracy: 0.5752\n",
      "Epoch 275/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8294 - accuracy: 0.7240 - val_loss: 1.3548 - val_accuracy: 0.4956\n",
      "Epoch 276/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8435 - accuracy: 0.7359 - val_loss: 1.2670 - val_accuracy: 0.5398\n",
      "Epoch 277/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8690 - accuracy: 0.7181 - val_loss: 1.2369 - val_accuracy: 0.5575\n",
      "Epoch 278/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8606 - accuracy: 0.7300 - val_loss: 1.2409 - val_accuracy: 0.5221\n",
      "Epoch 279/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8589 - accuracy: 0.7211 - val_loss: 1.2476 - val_accuracy: 0.5575\n",
      "Epoch 280/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.8425 - accuracy: 0.7270 - val_loss: 1.2170 - val_accuracy: 0.5752\n",
      "Epoch 281/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8303 - accuracy: 0.7240 - val_loss: 1.1658 - val_accuracy: 0.5841\n",
      "Epoch 282/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8061 - accuracy: 0.7240 - val_loss: 1.2060 - val_accuracy: 0.5575\n",
      "Epoch 283/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7905 - accuracy: 0.7329 - val_loss: 1.2842 - val_accuracy: 0.5575\n",
      "Epoch 284/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7874 - accuracy: 0.7329 - val_loss: 1.2103 - val_accuracy: 0.5752\n",
      "Epoch 285/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8212 - accuracy: 0.7181 - val_loss: 1.2494 - val_accuracy: 0.5221\n",
      "Epoch 286/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7953 - accuracy: 0.7270 - val_loss: 1.2768 - val_accuracy: 0.5398\n",
      "Epoch 287/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7897 - accuracy: 0.7300 - val_loss: 1.3013 - val_accuracy: 0.5664\n",
      "Epoch 288/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7882 - accuracy: 0.7448 - val_loss: 1.2458 - val_accuracy: 0.5575\n",
      "Epoch 289/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7799 - accuracy: 0.7359 - val_loss: 1.2910 - val_accuracy: 0.5752\n",
      "Epoch 290/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7902 - accuracy: 0.7389 - val_loss: 1.3098 - val_accuracy: 0.5575\n",
      "Epoch 291/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7986 - accuracy: 0.7359 - val_loss: 1.2740 - val_accuracy: 0.5575\n",
      "Epoch 292/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7788 - accuracy: 0.7418 - val_loss: 1.2877 - val_accuracy: 0.5664\n",
      "Epoch 293/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7905 - accuracy: 0.7329 - val_loss: 1.2242 - val_accuracy: 0.5841\n",
      "Epoch 294/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7928 - accuracy: 0.7359 - val_loss: 1.1971 - val_accuracy: 0.5664\n",
      "Epoch 295/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8075 - accuracy: 0.7329 - val_loss: 1.1909 - val_accuracy: 0.5929\n",
      "Epoch 296/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7874 - accuracy: 0.7448 - val_loss: 1.2129 - val_accuracy: 0.5487\n",
      "Epoch 297/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7752 - accuracy: 0.7448 - val_loss: 1.2464 - val_accuracy: 0.5752\n",
      "Epoch 298/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7754 - accuracy: 0.7418 - val_loss: 1.2810 - val_accuracy: 0.5752\n",
      "Epoch 299/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7846 - accuracy: 0.7418 - val_loss: 1.2405 - val_accuracy: 0.6106\n",
      "Epoch 300/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7784 - accuracy: 0.7418 - val_loss: 1.2581 - val_accuracy: 0.5841\n",
      "Epoch 301/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7747 - accuracy: 0.7537 - val_loss: 1.3103 - val_accuracy: 0.5841\n",
      "Epoch 302/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7869 - accuracy: 0.7478 - val_loss: 1.2954 - val_accuracy: 0.5841\n",
      "Epoch 303/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7829 - accuracy: 0.7507 - val_loss: 1.2939 - val_accuracy: 0.5664\n",
      "Epoch 304/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7842 - accuracy: 0.7507 - val_loss: 1.2958 - val_accuracy: 0.5752\n",
      "Epoch 305/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7966 - accuracy: 0.7507 - val_loss: 1.2423 - val_accuracy: 0.5841\n",
      "Epoch 306/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8026 - accuracy: 0.7359 - val_loss: 1.2741 - val_accuracy: 0.5664\n",
      "Epoch 307/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8056 - accuracy: 0.7389 - val_loss: 1.2758 - val_accuracy: 0.5310\n",
      "Epoch 308/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7920 - accuracy: 0.7537 - val_loss: 1.2793 - val_accuracy: 0.5664\n",
      "Epoch 309/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7967 - accuracy: 0.7448 - val_loss: 1.2472 - val_accuracy: 0.5664\n",
      "Epoch 310/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7828 - accuracy: 0.7478 - val_loss: 1.2737 - val_accuracy: 0.5575\n",
      "Epoch 311/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7858 - accuracy: 0.7507 - val_loss: 1.3106 - val_accuracy: 0.5575\n",
      "Epoch 312/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7761 - accuracy: 0.7418 - val_loss: 1.2952 - val_accuracy: 0.5664\n",
      "Epoch 313/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7568 - accuracy: 0.7418 - val_loss: 1.3312 - val_accuracy: 0.5929\n",
      "Epoch 314/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7593 - accuracy: 0.7596 - val_loss: 1.3726 - val_accuracy: 0.5575\n",
      "Epoch 315/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7648 - accuracy: 0.7596 - val_loss: 1.3538 - val_accuracy: 0.5398\n",
      "Epoch 316/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7659 - accuracy: 0.7507 - val_loss: 1.3560 - val_accuracy: 0.5398\n",
      "Epoch 317/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7319 - accuracy: 0.7596 - val_loss: 1.2997 - val_accuracy: 0.5752\n",
      "Epoch 318/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7474 - accuracy: 0.7507 - val_loss: 1.3256 - val_accuracy: 0.5575\n",
      "Epoch 319/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7511 - accuracy: 0.7507 - val_loss: 1.2813 - val_accuracy: 0.5133\n",
      "Epoch 320/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7589 - accuracy: 0.7507 - val_loss: 1.3808 - val_accuracy: 0.5221\n",
      "Epoch 321/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7587 - accuracy: 0.7626 - val_loss: 1.6101 - val_accuracy: 0.4779\n",
      "Epoch 322/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7639 - accuracy: 0.7448 - val_loss: 1.3044 - val_accuracy: 0.5310\n",
      "Epoch 323/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7704 - accuracy: 0.7537 - val_loss: 1.3639 - val_accuracy: 0.5310\n",
      "Epoch 324/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7725 - accuracy: 0.7329 - val_loss: 1.4778 - val_accuracy: 0.5487\n",
      "Epoch 325/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.8343 - accuracy: 0.7418 - val_loss: 1.4356 - val_accuracy: 0.5575\n",
      "Epoch 326/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7940 - accuracy: 0.7418 - val_loss: 1.5703 - val_accuracy: 0.4956\n",
      "Epoch 327/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7794 - accuracy: 0.7478 - val_loss: 1.4134 - val_accuracy: 0.4956\n",
      "Epoch 328/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7919 - accuracy: 0.7389 - val_loss: 1.4245 - val_accuracy: 0.4867\n",
      "Epoch 329/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7623 - accuracy: 0.7596 - val_loss: 1.4500 - val_accuracy: 0.5221\n",
      "Epoch 330/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7672 - accuracy: 0.7478 - val_loss: 1.4162 - val_accuracy: 0.4956\n",
      "Epoch 331/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7951 - accuracy: 0.7359 - val_loss: 1.3824 - val_accuracy: 0.5398\n",
      "Epoch 332/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7708 - accuracy: 0.7389 - val_loss: 1.3277 - val_accuracy: 0.5221\n",
      "Epoch 333/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7695 - accuracy: 0.7448 - val_loss: 1.3571 - val_accuracy: 0.5310\n",
      "Epoch 334/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7529 - accuracy: 0.7448 - val_loss: 1.3824 - val_accuracy: 0.5398\n",
      "Epoch 335/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7689 - accuracy: 0.7448 - val_loss: 1.5799 - val_accuracy: 0.5044\n",
      "Epoch 336/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7640 - accuracy: 0.7537 - val_loss: 1.5450 - val_accuracy: 0.4779\n",
      "Epoch 337/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7511 - accuracy: 0.7537 - val_loss: 1.3982 - val_accuracy: 0.5310\n",
      "Epoch 338/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7448 - accuracy: 0.7685 - val_loss: 1.4372 - val_accuracy: 0.5575\n",
      "Epoch 339/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7544 - accuracy: 0.7448 - val_loss: 1.5580 - val_accuracy: 0.4956\n",
      "Epoch 340/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7337 - accuracy: 0.7596 - val_loss: 1.5641 - val_accuracy: 0.5133\n",
      "Epoch 341/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7252 - accuracy: 0.7715 - val_loss: 1.5599 - val_accuracy: 0.5221\n",
      "Epoch 342/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7187 - accuracy: 0.7596 - val_loss: 1.7753 - val_accuracy: 0.4602\n",
      "Epoch 343/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7281 - accuracy: 0.7567 - val_loss: 1.6024 - val_accuracy: 0.5133\n",
      "Epoch 344/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7480 - accuracy: 0.7448 - val_loss: 1.4235 - val_accuracy: 0.5221\n",
      "Epoch 345/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7389 - accuracy: 0.7478 - val_loss: 1.4369 - val_accuracy: 0.5575\n",
      "Epoch 346/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7289 - accuracy: 0.7478 - val_loss: 1.4191 - val_accuracy: 0.5487\n",
      "Epoch 347/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7215 - accuracy: 0.7507 - val_loss: 1.3883 - val_accuracy: 0.5221\n",
      "Epoch 348/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7770 - accuracy: 0.7359 - val_loss: 1.4492 - val_accuracy: 0.5398\n",
      "Epoch 349/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7754 - accuracy: 0.7151 - val_loss: 1.4295 - val_accuracy: 0.5841\n",
      "Epoch 350/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7576 - accuracy: 0.7359 - val_loss: 1.4361 - val_accuracy: 0.5575\n",
      "Epoch 351/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7799 - accuracy: 0.7389 - val_loss: 1.3833 - val_accuracy: 0.5929\n",
      "Epoch 352/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7690 - accuracy: 0.7507 - val_loss: 1.4388 - val_accuracy: 0.5575\n",
      "Epoch 353/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7202 - accuracy: 0.7596 - val_loss: 1.5076 - val_accuracy: 0.5221\n",
      "Epoch 354/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7651 - accuracy: 0.7656 - val_loss: 1.4598 - val_accuracy: 0.5487\n",
      "Epoch 355/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7601 - accuracy: 0.7478 - val_loss: 1.4898 - val_accuracy: 0.5398\n",
      "Epoch 356/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7482 - accuracy: 0.7567 - val_loss: 1.4921 - val_accuracy: 0.5133\n",
      "Epoch 357/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7096 - accuracy: 0.7656 - val_loss: 1.4469 - val_accuracy: 0.5221\n",
      "Epoch 358/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7341 - accuracy: 0.7567 - val_loss: 1.3534 - val_accuracy: 0.5841\n",
      "Epoch 359/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7373 - accuracy: 0.7507 - val_loss: 1.3706 - val_accuracy: 0.5575\n",
      "Epoch 360/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7213 - accuracy: 0.7507 - val_loss: 1.3979 - val_accuracy: 0.5487\n",
      "Epoch 361/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7327 - accuracy: 0.7596 - val_loss: 1.3737 - val_accuracy: 0.5310\n",
      "Epoch 362/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7464 - accuracy: 0.7537 - val_loss: 1.4626 - val_accuracy: 0.4956\n",
      "Epoch 363/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7618 - accuracy: 0.7537 - val_loss: 1.4260 - val_accuracy: 0.5398\n",
      "Epoch 364/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7755 - accuracy: 0.7418 - val_loss: 1.4576 - val_accuracy: 0.5398\n",
      "Epoch 365/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7653 - accuracy: 0.7537 - val_loss: 1.5036 - val_accuracy: 0.5044\n",
      "Epoch 366/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7492 - accuracy: 0.7537 - val_loss: 1.4421 - val_accuracy: 0.4956\n",
      "Epoch 367/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7162 - accuracy: 0.7656 - val_loss: 1.3632 - val_accuracy: 0.5221\n",
      "Epoch 368/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7275 - accuracy: 0.7537 - val_loss: 1.4847 - val_accuracy: 0.5133\n",
      "Epoch 369/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7267 - accuracy: 0.7418 - val_loss: 1.5834 - val_accuracy: 0.4867\n",
      "Epoch 370/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7576 - accuracy: 0.7478 - val_loss: 1.4349 - val_accuracy: 0.5221\n",
      "Epoch 371/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7330 - accuracy: 0.7537 - val_loss: 1.3677 - val_accuracy: 0.5044\n",
      "Epoch 372/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7185 - accuracy: 0.7478 - val_loss: 1.4420 - val_accuracy: 0.5133\n",
      "Epoch 373/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7030 - accuracy: 0.7626 - val_loss: 1.4651 - val_accuracy: 0.5133\n",
      "Epoch 374/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7086 - accuracy: 0.7537 - val_loss: 1.3944 - val_accuracy: 0.4779\n",
      "Epoch 375/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6941 - accuracy: 0.7656 - val_loss: 1.4009 - val_accuracy: 0.5310\n",
      "Epoch 376/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6877 - accuracy: 0.7715 - val_loss: 1.6596 - val_accuracy: 0.4248\n",
      "Epoch 377/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6860 - accuracy: 0.7685 - val_loss: 1.4843 - val_accuracy: 0.4779\n",
      "Epoch 378/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7156 - accuracy: 0.7715 - val_loss: 1.4738 - val_accuracy: 0.4690\n",
      "Epoch 379/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7095 - accuracy: 0.7774 - val_loss: 1.4051 - val_accuracy: 0.4867\n",
      "Epoch 380/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6984 - accuracy: 0.7567 - val_loss: 1.5554 - val_accuracy: 0.4779\n",
      "Epoch 381/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6899 - accuracy: 0.7656 - val_loss: 1.5790 - val_accuracy: 0.4690\n",
      "Epoch 382/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7142 - accuracy: 0.7656 - val_loss: 1.5494 - val_accuracy: 0.4602\n",
      "Epoch 383/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7011 - accuracy: 0.7715 - val_loss: 1.5966 - val_accuracy: 0.4956\n",
      "Epoch 384/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6920 - accuracy: 0.7685 - val_loss: 1.4958 - val_accuracy: 0.5310\n",
      "Epoch 385/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7008 - accuracy: 0.7834 - val_loss: 1.4435 - val_accuracy: 0.5133\n",
      "Epoch 386/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7114 - accuracy: 0.7715 - val_loss: 1.4853 - val_accuracy: 0.5133\n",
      "Epoch 387/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6956 - accuracy: 0.7745 - val_loss: 1.5248 - val_accuracy: 0.5133\n",
      "Epoch 388/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7478 - accuracy: 0.7418 - val_loss: 1.4301 - val_accuracy: 0.5133\n",
      "Epoch 389/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7193 - accuracy: 0.7685 - val_loss: 1.4449 - val_accuracy: 0.5398\n",
      "Epoch 390/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6783 - accuracy: 0.7626 - val_loss: 1.4605 - val_accuracy: 0.5310\n",
      "Epoch 391/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6837 - accuracy: 0.7626 - val_loss: 1.5699 - val_accuracy: 0.4867\n",
      "Epoch 392/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7043 - accuracy: 0.7478 - val_loss: 1.5179 - val_accuracy: 0.4956\n",
      "Epoch 393/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7009 - accuracy: 0.7596 - val_loss: 1.4816 - val_accuracy: 0.4602\n",
      "Epoch 394/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7011 - accuracy: 0.7478 - val_loss: 1.4329 - val_accuracy: 0.4779\n",
      "Epoch 395/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6671 - accuracy: 0.7715 - val_loss: 1.4723 - val_accuracy: 0.4690\n",
      "Epoch 396/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6840 - accuracy: 0.7596 - val_loss: 1.5332 - val_accuracy: 0.4779\n",
      "Epoch 397/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7001 - accuracy: 0.7656 - val_loss: 1.5684 - val_accuracy: 0.4690\n",
      "Epoch 398/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6682 - accuracy: 0.7715 - val_loss: 1.6168 - val_accuracy: 0.4867\n",
      "Epoch 399/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6901 - accuracy: 0.7715 - val_loss: 1.8667 - val_accuracy: 0.4336\n",
      "Epoch 400/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6746 - accuracy: 0.7804 - val_loss: 1.5993 - val_accuracy: 0.5133\n",
      "Epoch 401/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6816 - accuracy: 0.7745 - val_loss: 1.5480 - val_accuracy: 0.4956\n",
      "Epoch 402/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7053 - accuracy: 0.7715 - val_loss: 1.8091 - val_accuracy: 0.4602\n",
      "Epoch 403/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7099 - accuracy: 0.7418 - val_loss: 2.8167 - val_accuracy: 0.3009\n",
      "Epoch 404/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7094 - accuracy: 0.7507 - val_loss: 1.9545 - val_accuracy: 0.4779\n",
      "Epoch 405/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6927 - accuracy: 0.7715 - val_loss: 1.6117 - val_accuracy: 0.5044\n",
      "Epoch 406/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6762 - accuracy: 0.7685 - val_loss: 1.6419 - val_accuracy: 0.4956\n",
      "Epoch 407/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6860 - accuracy: 0.7656 - val_loss: 1.4743 - val_accuracy: 0.4690\n",
      "Epoch 408/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6866 - accuracy: 0.7715 - val_loss: 1.4679 - val_accuracy: 0.4867\n",
      "Epoch 409/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6365 - accuracy: 0.7774 - val_loss: 1.5475 - val_accuracy: 0.4956\n",
      "Epoch 410/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6665 - accuracy: 0.7834 - val_loss: 1.4864 - val_accuracy: 0.5398\n",
      "Epoch 411/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6677 - accuracy: 0.7774 - val_loss: 1.5208 - val_accuracy: 0.5310\n",
      "Epoch 412/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6512 - accuracy: 0.7774 - val_loss: 1.5401 - val_accuracy: 0.4336\n",
      "Epoch 413/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6628 - accuracy: 0.7953 - val_loss: 1.5406 - val_accuracy: 0.5221\n",
      "Epoch 414/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6920 - accuracy: 0.7745 - val_loss: 1.5201 - val_accuracy: 0.5398\n",
      "Epoch 415/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6741 - accuracy: 0.7804 - val_loss: 1.7297 - val_accuracy: 0.4425\n",
      "Epoch 416/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6595 - accuracy: 0.7834 - val_loss: 1.4441 - val_accuracy: 0.5221\n",
      "Epoch 417/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6650 - accuracy: 0.7864 - val_loss: 1.4938 - val_accuracy: 0.5221\n",
      "Epoch 418/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6531 - accuracy: 0.7923 - val_loss: 1.4204 - val_accuracy: 0.5398\n",
      "Epoch 419/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6508 - accuracy: 0.8012 - val_loss: 1.4150 - val_accuracy: 0.5310\n",
      "Epoch 420/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6326 - accuracy: 0.7923 - val_loss: 1.5086 - val_accuracy: 0.5221\n",
      "Epoch 421/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6414 - accuracy: 0.7893 - val_loss: 1.4056 - val_accuracy: 0.5310\n",
      "Epoch 422/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6338 - accuracy: 0.7715 - val_loss: 1.4337 - val_accuracy: 0.5487\n",
      "Epoch 423/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6472 - accuracy: 0.7864 - val_loss: 8.1168 - val_accuracy: 0.2301\n",
      "Epoch 424/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6834 - accuracy: 0.7478 - val_loss: 1.4075 - val_accuracy: 0.5487\n",
      "Epoch 425/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6903 - accuracy: 0.7804 - val_loss: 1.4578 - val_accuracy: 0.5841\n",
      "Epoch 426/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6718 - accuracy: 0.7864 - val_loss: 1.4460 - val_accuracy: 0.5221\n",
      "Epoch 427/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6866 - accuracy: 0.7656 - val_loss: 1.3639 - val_accuracy: 0.5487\n",
      "Epoch 428/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7069 - accuracy: 0.7715 - val_loss: 1.5139 - val_accuracy: 0.5398\n",
      "Epoch 429/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6848 - accuracy: 0.7923 - val_loss: 1.5835 - val_accuracy: 0.5487\n",
      "Epoch 430/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6884 - accuracy: 0.7804 - val_loss: 1.5242 - val_accuracy: 0.5487\n",
      "Epoch 431/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7007 - accuracy: 0.7715 - val_loss: 1.5262 - val_accuracy: 0.5310\n",
      "Epoch 432/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6954 - accuracy: 0.7893 - val_loss: 1.6238 - val_accuracy: 0.5044\n",
      "Epoch 433/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6847 - accuracy: 0.7804 - val_loss: 1.6155 - val_accuracy: 0.5398\n",
      "Epoch 434/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6731 - accuracy: 0.7864 - val_loss: 1.5470 - val_accuracy: 0.5487\n",
      "Epoch 435/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6548 - accuracy: 0.7953 - val_loss: 1.5307 - val_accuracy: 0.5133\n",
      "Epoch 436/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 3.3328 - accuracy: 0.7092 - val_loss: 1.8364 - val_accuracy: 0.4956\n",
      "Epoch 437/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6567 - accuracy: 0.7685 - val_loss: 1.5366 - val_accuracy: 0.5133\n",
      "Epoch 438/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6509 - accuracy: 0.7745 - val_loss: 1.5231 - val_accuracy: 0.5221\n",
      "Epoch 439/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6465 - accuracy: 0.7804 - val_loss: 1.5132 - val_accuracy: 0.5221\n",
      "Epoch 440/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6563 - accuracy: 0.7685 - val_loss: 1.4627 - val_accuracy: 0.5310\n",
      "Epoch 441/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6377 - accuracy: 0.7864 - val_loss: 1.4292 - val_accuracy: 0.5221\n",
      "Epoch 442/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6505 - accuracy: 0.7834 - val_loss: 1.4956 - val_accuracy: 0.5310\n",
      "Epoch 443/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6509 - accuracy: 0.7923 - val_loss: 1.5078 - val_accuracy: 0.5310\n",
      "Epoch 444/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6527 - accuracy: 0.7923 - val_loss: 1.5144 - val_accuracy: 0.5310\n",
      "Epoch 445/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6496 - accuracy: 0.7953 - val_loss: 1.5025 - val_accuracy: 0.5575\n",
      "Epoch 446/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6813 - accuracy: 0.7864 - val_loss: 1.5091 - val_accuracy: 0.5310\n",
      "Epoch 447/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6833 - accuracy: 0.7953 - val_loss: 1.5416 - val_accuracy: 0.5398\n",
      "Epoch 448/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6931 - accuracy: 0.7982 - val_loss: 1.5513 - val_accuracy: 0.5133\n",
      "Epoch 449/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6855 - accuracy: 0.7923 - val_loss: 1.4214 - val_accuracy: 0.5044\n",
      "Epoch 450/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6903 - accuracy: 0.7745 - val_loss: 1.3730 - val_accuracy: 0.5133\n",
      "Epoch 451/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7095 - accuracy: 0.7774 - val_loss: 1.3460 - val_accuracy: 0.5841\n",
      "Epoch 452/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6932 - accuracy: 0.7715 - val_loss: 1.4261 - val_accuracy: 0.5310\n",
      "Epoch 453/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6581 - accuracy: 0.7804 - val_loss: 1.3666 - val_accuracy: 0.5133\n",
      "Epoch 454/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6642 - accuracy: 0.7864 - val_loss: 1.8619 - val_accuracy: 0.5221\n",
      "Epoch 455/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7244 - accuracy: 0.7685 - val_loss: 1.7102 - val_accuracy: 0.4779\n",
      "Epoch 456/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7142 - accuracy: 0.7804 - val_loss: 1.6920 - val_accuracy: 0.5310\n",
      "Epoch 457/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6835 - accuracy: 0.7834 - val_loss: 1.5677 - val_accuracy: 0.5221\n",
      "Epoch 458/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6684 - accuracy: 0.7745 - val_loss: 1.6537 - val_accuracy: 0.5044\n",
      "Epoch 459/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7070 - accuracy: 0.7715 - val_loss: 1.7196 - val_accuracy: 0.4779\n",
      "Epoch 460/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.7285 - accuracy: 0.7774 - val_loss: 2.0476 - val_accuracy: 0.4425\n",
      "Epoch 461/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7222 - accuracy: 0.7656 - val_loss: 21.4933 - val_accuracy: 0.2566\n",
      "Epoch 462/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7207 - accuracy: 0.7834 - val_loss: 22.9147 - val_accuracy: 0.2566\n",
      "Epoch 463/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7038 - accuracy: 0.7745 - val_loss: 2.0878 - val_accuracy: 0.4425\n",
      "Epoch 464/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7146 - accuracy: 0.7567 - val_loss: 1.9579 - val_accuracy: 0.4779\n",
      "Epoch 465/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7362 - accuracy: 0.7626 - val_loss: 1.7316 - val_accuracy: 0.5133\n",
      "Epoch 466/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7157 - accuracy: 0.7745 - val_loss: 1.5322 - val_accuracy: 0.5310\n",
      "Epoch 467/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6792 - accuracy: 0.7864 - val_loss: 1.6788 - val_accuracy: 0.4956\n",
      "Epoch 468/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6711 - accuracy: 0.7982 - val_loss: 1.6176 - val_accuracy: 0.4956\n",
      "Epoch 469/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6436 - accuracy: 0.8071 - val_loss: 2.0990 - val_accuracy: 0.4425\n",
      "Epoch 470/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6861 - accuracy: 0.7923 - val_loss: 1.8528 - val_accuracy: 0.4779\n",
      "Epoch 471/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7082 - accuracy: 0.7774 - val_loss: 1.8160 - val_accuracy: 0.5133\n",
      "Epoch 472/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6935 - accuracy: 0.7715 - val_loss: 1.6997 - val_accuracy: 0.5221\n",
      "Epoch 473/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6871 - accuracy: 0.7893 - val_loss: 1.5857 - val_accuracy: 0.5310\n",
      "Epoch 474/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6991 - accuracy: 0.7834 - val_loss: 1.6400 - val_accuracy: 0.5133\n",
      "Epoch 475/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6953 - accuracy: 0.7893 - val_loss: 1.6882 - val_accuracy: 0.4867\n",
      "Epoch 476/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6929 - accuracy: 0.7864 - val_loss: 1.5780 - val_accuracy: 0.5133\n",
      "Epoch 477/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6832 - accuracy: 0.7834 - val_loss: 1.5660 - val_accuracy: 0.5133\n",
      "Epoch 478/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6994 - accuracy: 0.7834 - val_loss: 1.5141 - val_accuracy: 0.4956\n",
      "Epoch 479/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6824 - accuracy: 0.7834 - val_loss: 1.4708 - val_accuracy: 0.5044\n",
      "Epoch 480/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6751 - accuracy: 0.7893 - val_loss: 1.4515 - val_accuracy: 0.5487\n",
      "Epoch 481/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7136 - accuracy: 0.7745 - val_loss: 1.6671 - val_accuracy: 0.4867\n",
      "Epoch 482/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7063 - accuracy: 0.7745 - val_loss: 1.7777 - val_accuracy: 0.4513\n",
      "Epoch 483/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6827 - accuracy: 0.7804 - val_loss: 1.6710 - val_accuracy: 0.4779\n",
      "Epoch 484/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6628 - accuracy: 0.7774 - val_loss: 1.6268 - val_accuracy: 0.5221\n",
      "Epoch 485/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6668 - accuracy: 0.7923 - val_loss: 1.7510 - val_accuracy: 0.4602\n",
      "Epoch 486/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6580 - accuracy: 0.7834 - val_loss: 1.5551 - val_accuracy: 0.4779\n",
      "Epoch 487/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6445 - accuracy: 0.7893 - val_loss: 1.6828 - val_accuracy: 0.4956\n",
      "Epoch 488/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6454 - accuracy: 0.7715 - val_loss: 1.7935 - val_accuracy: 0.4779\n",
      "Epoch 489/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6705 - accuracy: 0.7804 - val_loss: 1.6837 - val_accuracy: 0.4956\n",
      "Epoch 490/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6535 - accuracy: 0.7715 - val_loss: 1.7796 - val_accuracy: 0.5044\n",
      "Epoch 491/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6396 - accuracy: 0.7893 - val_loss: 1.9177 - val_accuracy: 0.3894\n",
      "Epoch 492/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6628 - accuracy: 0.8131 - val_loss: 1.7314 - val_accuracy: 0.4513\n",
      "Epoch 493/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6568 - accuracy: 0.7953 - val_loss: 2.0634 - val_accuracy: 0.4159\n",
      "Epoch 494/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6614 - accuracy: 0.8012 - val_loss: 1.8488 - val_accuracy: 0.4779\n",
      "Epoch 495/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6640 - accuracy: 0.7834 - val_loss: 1.8757 - val_accuracy: 0.4513\n",
      "Epoch 496/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7044 - accuracy: 0.7864 - val_loss: 1.9726 - val_accuracy: 0.4513\n",
      "Epoch 497/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6909 - accuracy: 0.7953 - val_loss: 2.1580 - val_accuracy: 0.4071\n",
      "Epoch 498/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6597 - accuracy: 0.7864 - val_loss: 1.9589 - val_accuracy: 0.4602\n",
      "Epoch 499/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6353 - accuracy: 0.7923 - val_loss: 1.9199 - val_accuracy: 0.4779\n",
      "Epoch 500/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6169 - accuracy: 0.8101 - val_loss: 1.9120 - val_accuracy: 0.4779\n",
      "Epoch 501/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6311 - accuracy: 0.8101 - val_loss: 1.8410 - val_accuracy: 0.5044\n",
      "Epoch 502/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6371 - accuracy: 0.8042 - val_loss: 1.7551 - val_accuracy: 0.4867\n",
      "Epoch 503/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6461 - accuracy: 0.8131 - val_loss: 1.7428 - val_accuracy: 0.4956\n",
      "Epoch 504/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6272 - accuracy: 0.8160 - val_loss: 1.7662 - val_accuracy: 0.4956\n",
      "Epoch 505/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6230 - accuracy: 0.8131 - val_loss: 1.8315 - val_accuracy: 0.4956\n",
      "Epoch 506/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6078 - accuracy: 0.8190 - val_loss: 2.2105 - val_accuracy: 0.4159\n",
      "Epoch 507/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7096 - accuracy: 0.7923 - val_loss: 1.9371 - val_accuracy: 0.5133\n",
      "Epoch 508/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6881 - accuracy: 0.7923 - val_loss: 1.7046 - val_accuracy: 0.4779\n",
      "Epoch 509/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6480 - accuracy: 0.8071 - val_loss: 1.7120 - val_accuracy: 0.5133\n",
      "Epoch 510/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7192 - accuracy: 0.7834 - val_loss: 1.7207 - val_accuracy: 0.5221\n",
      "Epoch 511/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7038 - accuracy: 0.7923 - val_loss: 1.8234 - val_accuracy: 0.5133\n",
      "Epoch 512/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6766 - accuracy: 0.7953 - val_loss: 1.8262 - val_accuracy: 0.5221\n",
      "Epoch 513/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6505 - accuracy: 0.7923 - val_loss: 1.8058 - val_accuracy: 0.5044\n",
      "Epoch 514/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6796 - accuracy: 0.7923 - val_loss: 1.8786 - val_accuracy: 0.4602\n",
      "Epoch 515/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6887 - accuracy: 0.7893 - val_loss: 1.9845 - val_accuracy: 0.4425\n",
      "Epoch 516/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6889 - accuracy: 0.7864 - val_loss: 2.1864 - val_accuracy: 0.4071\n",
      "Epoch 517/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6668 - accuracy: 0.7953 - val_loss: 2.2434 - val_accuracy: 0.3982\n",
      "Epoch 518/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6684 - accuracy: 0.7953 - val_loss: 2.9924 - val_accuracy: 0.3009\n",
      "Epoch 519/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6592 - accuracy: 0.7923 - val_loss: 1.8061 - val_accuracy: 0.4602\n",
      "Epoch 520/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6771 - accuracy: 0.8071 - val_loss: 1.8505 - val_accuracy: 0.4336\n",
      "Epoch 521/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6414 - accuracy: 0.7953 - val_loss: 2.1774 - val_accuracy: 0.3805\n",
      "Epoch 522/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6611 - accuracy: 0.7982 - val_loss: 2.8526 - val_accuracy: 0.3186\n",
      "Epoch 523/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6577 - accuracy: 0.8101 - val_loss: 2.2360 - val_accuracy: 0.4071\n",
      "Epoch 524/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6679 - accuracy: 0.7982 - val_loss: 1.9274 - val_accuracy: 0.5133\n",
      "Epoch 525/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6410 - accuracy: 0.7982 - val_loss: 1.7770 - val_accuracy: 0.5133\n",
      "Epoch 526/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6128 - accuracy: 0.8101 - val_loss: 1.7602 - val_accuracy: 0.4779\n",
      "Epoch 527/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6243 - accuracy: 0.7982 - val_loss: 1.7614 - val_accuracy: 0.4779\n",
      "Epoch 528/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6033 - accuracy: 0.8042 - val_loss: 1.7672 - val_accuracy: 0.5044\n",
      "Epoch 529/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5890 - accuracy: 0.8101 - val_loss: 1.6215 - val_accuracy: 0.5133\n",
      "Epoch 530/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5898 - accuracy: 0.8101 - val_loss: 1.5251 - val_accuracy: 0.5487\n",
      "Epoch 531/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6146 - accuracy: 0.7893 - val_loss: 1.5919 - val_accuracy: 0.5487\n",
      "Epoch 532/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6044 - accuracy: 0.8071 - val_loss: 1.5880 - val_accuracy: 0.5133\n",
      "Epoch 533/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5949 - accuracy: 0.8131 - val_loss: 1.5875 - val_accuracy: 0.4956\n",
      "Epoch 534/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6158 - accuracy: 0.8190 - val_loss: 1.6140 - val_accuracy: 0.5221\n",
      "Epoch 535/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6128 - accuracy: 0.8220 - val_loss: 1.7138 - val_accuracy: 0.5221\n",
      "Epoch 536/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6031 - accuracy: 0.8220 - val_loss: 1.6477 - val_accuracy: 0.5044\n",
      "Epoch 537/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6130 - accuracy: 0.8012 - val_loss: 1.6302 - val_accuracy: 0.4956\n",
      "Epoch 538/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6038 - accuracy: 0.8249 - val_loss: 1.7517 - val_accuracy: 0.4956\n",
      "Epoch 539/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5894 - accuracy: 0.8190 - val_loss: 1.7034 - val_accuracy: 0.5221\n",
      "Epoch 540/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6649 - accuracy: 0.8012 - val_loss: 1.8196 - val_accuracy: 0.4956\n",
      "Epoch 541/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6913 - accuracy: 0.7923 - val_loss: 1.7286 - val_accuracy: 0.4779\n",
      "Epoch 542/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.7156 - accuracy: 0.7864 - val_loss: 1.7536 - val_accuracy: 0.4602\n",
      "Epoch 543/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6839 - accuracy: 0.8071 - val_loss: 1.7579 - val_accuracy: 0.4425\n",
      "Epoch 544/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6875 - accuracy: 0.7656 - val_loss: 1.8499 - val_accuracy: 0.4867\n",
      "Epoch 545/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6740 - accuracy: 0.7893 - val_loss: 1.8773 - val_accuracy: 0.4425\n",
      "Epoch 546/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6670 - accuracy: 0.7834 - val_loss: 1.9919 - val_accuracy: 0.2832\n",
      "Epoch 547/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6493 - accuracy: 0.8012 - val_loss: 1.8967 - val_accuracy: 0.3540\n",
      "Epoch 548/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6586 - accuracy: 0.7715 - val_loss: 1.7289 - val_accuracy: 0.4425\n",
      "Epoch 549/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6155 - accuracy: 0.8131 - val_loss: 1.6960 - val_accuracy: 0.3894\n",
      "Epoch 550/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6122 - accuracy: 0.8131 - val_loss: 1.8665 - val_accuracy: 0.4513\n",
      "Epoch 551/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6378 - accuracy: 0.7834 - val_loss: 1.8012 - val_accuracy: 0.4071\n",
      "Epoch 552/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6303 - accuracy: 0.8101 - val_loss: 1.7514 - val_accuracy: 0.4867\n",
      "Epoch 553/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6239 - accuracy: 0.7923 - val_loss: 1.7550 - val_accuracy: 0.4513\n",
      "Epoch 554/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6065 - accuracy: 0.8160 - val_loss: 1.6793 - val_accuracy: 0.5133\n",
      "Epoch 555/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6392 - accuracy: 0.7982 - val_loss: 1.7203 - val_accuracy: 0.4690\n",
      "Epoch 556/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6536 - accuracy: 0.8042 - val_loss: 1.6512 - val_accuracy: 0.4690\n",
      "Epoch 557/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6307 - accuracy: 0.8101 - val_loss: 1.5829 - val_accuracy: 0.4867\n",
      "Epoch 558/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6588 - accuracy: 0.8012 - val_loss: 1.7316 - val_accuracy: 0.4956\n",
      "Epoch 559/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6311 - accuracy: 0.7982 - val_loss: 1.8144 - val_accuracy: 0.4779\n",
      "Epoch 560/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6357 - accuracy: 0.8042 - val_loss: 1.9185 - val_accuracy: 0.4602\n",
      "Epoch 561/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6155 - accuracy: 0.8101 - val_loss: 1.8355 - val_accuracy: 0.4602\n",
      "Epoch 562/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5867 - accuracy: 0.8071 - val_loss: 1.7886 - val_accuracy: 0.4779\n",
      "Epoch 563/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6094 - accuracy: 0.8190 - val_loss: 1.7905 - val_accuracy: 0.4690\n",
      "Epoch 564/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5850 - accuracy: 0.8190 - val_loss: 1.7714 - val_accuracy: 0.4425\n",
      "Epoch 565/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5909 - accuracy: 0.8042 - val_loss: 1.6853 - val_accuracy: 0.4602\n",
      "Epoch 566/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5691 - accuracy: 0.8220 - val_loss: 1.7847 - val_accuracy: 0.4956\n",
      "Epoch 567/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5550 - accuracy: 0.8309 - val_loss: 2.0469 - val_accuracy: 0.4513\n",
      "Epoch 568/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5790 - accuracy: 0.8249 - val_loss: 1.9046 - val_accuracy: 0.4159\n",
      "Epoch 569/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6084 - accuracy: 0.8160 - val_loss: 2.1669 - val_accuracy: 0.4513\n",
      "Epoch 570/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6038 - accuracy: 0.8279 - val_loss: 1.8935 - val_accuracy: 0.4602\n",
      "Epoch 571/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5935 - accuracy: 0.8131 - val_loss: 1.8290 - val_accuracy: 0.4779\n",
      "Epoch 572/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5927 - accuracy: 0.8131 - val_loss: 1.7930 - val_accuracy: 0.4867\n",
      "Epoch 573/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5789 - accuracy: 0.8309 - val_loss: 1.7975 - val_accuracy: 0.4867\n",
      "Epoch 574/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6173 - accuracy: 0.7864 - val_loss: 1.7552 - val_accuracy: 0.4867\n",
      "Epoch 575/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5929 - accuracy: 0.8190 - val_loss: 1.7885 - val_accuracy: 0.5044\n",
      "Epoch 576/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5963 - accuracy: 0.8160 - val_loss: 1.8355 - val_accuracy: 0.4867\n",
      "Epoch 577/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6047 - accuracy: 0.8101 - val_loss: 1.6708 - val_accuracy: 0.5044\n",
      "Epoch 578/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6107 - accuracy: 0.8131 - val_loss: 1.6749 - val_accuracy: 0.4602\n",
      "Epoch 579/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5979 - accuracy: 0.8131 - val_loss: 1.6473 - val_accuracy: 0.5487\n",
      "Epoch 580/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5764 - accuracy: 0.8190 - val_loss: 1.6250 - val_accuracy: 0.5310\n",
      "Epoch 581/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5961 - accuracy: 0.8160 - val_loss: 1.6641 - val_accuracy: 0.5044\n",
      "Epoch 582/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6422 - accuracy: 0.8071 - val_loss: 1.7672 - val_accuracy: 0.4956\n",
      "Epoch 583/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6031 - accuracy: 0.8190 - val_loss: 1.8704 - val_accuracy: 0.4690\n",
      "Epoch 584/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5969 - accuracy: 0.8190 - val_loss: 2.0138 - val_accuracy: 0.3982\n",
      "Epoch 585/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6714 - accuracy: 0.7923 - val_loss: 1.8267 - val_accuracy: 0.4513\n",
      "Epoch 586/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5828 - accuracy: 0.8279 - val_loss: 1.6058 - val_accuracy: 0.4425\n",
      "Epoch 587/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6143 - accuracy: 0.8190 - val_loss: 1.8829 - val_accuracy: 0.4336\n",
      "Epoch 588/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5982 - accuracy: 0.8160 - val_loss: 1.8122 - val_accuracy: 0.4248\n",
      "Epoch 589/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5914 - accuracy: 0.8131 - val_loss: 1.6544 - val_accuracy: 0.4867\n",
      "Epoch 590/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5631 - accuracy: 0.8101 - val_loss: 1.9042 - val_accuracy: 0.4867\n",
      "Epoch 591/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5959 - accuracy: 0.8249 - val_loss: 2.1135 - val_accuracy: 0.3982\n",
      "Epoch 592/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6319 - accuracy: 0.8042 - val_loss: 2.2880 - val_accuracy: 0.4159\n",
      "Epoch 593/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6218 - accuracy: 0.8279 - val_loss: 2.1382 - val_accuracy: 0.4513\n",
      "Epoch 594/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.6296 - accuracy: 0.8190 - val_loss: 2.0272 - val_accuracy: 0.4602\n",
      "Epoch 595/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6137 - accuracy: 0.8309 - val_loss: 1.8948 - val_accuracy: 0.4779\n",
      "Epoch 596/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6116 - accuracy: 0.8309 - val_loss: 2.0386 - val_accuracy: 0.3982\n",
      "Epoch 597/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6293 - accuracy: 0.8160 - val_loss: 1.8467 - val_accuracy: 0.5044\n",
      "Epoch 598/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5498 - accuracy: 0.8309 - val_loss: 1.8986 - val_accuracy: 0.5221\n",
      "Epoch 599/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5717 - accuracy: 0.8249 - val_loss: 1.7599 - val_accuracy: 0.5487\n",
      "Epoch 600/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5879 - accuracy: 0.8190 - val_loss: 1.7868 - val_accuracy: 0.5310\n",
      "Epoch 601/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5658 - accuracy: 0.8220 - val_loss: 28.1073 - val_accuracy: 0.2566\n",
      "Epoch 602/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5913 - accuracy: 0.8190 - val_loss: 13.4891 - val_accuracy: 0.2301\n",
      "Epoch 603/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5802 - accuracy: 0.8249 - val_loss: 2.0700 - val_accuracy: 0.4159\n",
      "Epoch 604/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5677 - accuracy: 0.8309 - val_loss: 1.8665 - val_accuracy: 0.4779\n",
      "Epoch 605/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5589 - accuracy: 0.8309 - val_loss: 1.7691 - val_accuracy: 0.5221\n",
      "Epoch 606/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5568 - accuracy: 0.8190 - val_loss: 1.7630 - val_accuracy: 0.5398\n",
      "Epoch 607/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5337 - accuracy: 0.8368 - val_loss: 1.8706 - val_accuracy: 0.5044\n",
      "Epoch 608/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5511 - accuracy: 0.8249 - val_loss: 1.9141 - val_accuracy: 0.5398\n",
      "Epoch 609/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5460 - accuracy: 0.8368 - val_loss: 1.8336 - val_accuracy: 0.4956\n",
      "Epoch 610/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5628 - accuracy: 0.8279 - val_loss: 1.7593 - val_accuracy: 0.4779\n",
      "Epoch 611/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5664 - accuracy: 0.8338 - val_loss: 1.7566 - val_accuracy: 0.5221\n",
      "Epoch 612/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5639 - accuracy: 0.8368 - val_loss: 1.8006 - val_accuracy: 0.5133\n",
      "Epoch 613/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5603 - accuracy: 0.8309 - val_loss: 1.8644 - val_accuracy: 0.5133\n",
      "Epoch 614/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5447 - accuracy: 0.8338 - val_loss: 1.7796 - val_accuracy: 0.5044\n",
      "Epoch 615/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5975 - accuracy: 0.7982 - val_loss: 1.8234 - val_accuracy: 0.4867\n",
      "Epoch 616/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6020 - accuracy: 0.8131 - val_loss: 1.7021 - val_accuracy: 0.4867\n",
      "Epoch 617/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5444 - accuracy: 0.8309 - val_loss: 1.7538 - val_accuracy: 0.4602\n",
      "Epoch 618/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5563 - accuracy: 0.8338 - val_loss: 1.8906 - val_accuracy: 0.4867\n",
      "Epoch 619/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5482 - accuracy: 0.8427 - val_loss: 1.6849 - val_accuracy: 0.4867\n",
      "Epoch 620/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5557 - accuracy: 0.8338 - val_loss: 1.5951 - val_accuracy: 0.5221\n",
      "Epoch 621/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5621 - accuracy: 0.8427 - val_loss: 1.6802 - val_accuracy: 0.5044\n",
      "Epoch 622/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5583 - accuracy: 0.8249 - val_loss: 1.5390 - val_accuracy: 0.5221\n",
      "Epoch 623/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5322 - accuracy: 0.8338 - val_loss: 1.5139 - val_accuracy: 0.5044\n",
      "Epoch 624/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5378 - accuracy: 0.8338 - val_loss: 1.8615 - val_accuracy: 0.3097\n",
      "Epoch 625/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5614 - accuracy: 0.8338 - val_loss: 1.5108 - val_accuracy: 0.5133\n",
      "Epoch 626/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5446 - accuracy: 0.8368 - val_loss: 1.6095 - val_accuracy: 0.5221\n",
      "Epoch 627/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5372 - accuracy: 0.8279 - val_loss: 1.7232 - val_accuracy: 0.4425\n",
      "Epoch 628/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5443 - accuracy: 0.8368 - val_loss: 1.7365 - val_accuracy: 0.4425\n",
      "Epoch 629/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5555 - accuracy: 0.8487 - val_loss: 1.8879 - val_accuracy: 0.4248\n",
      "Epoch 630/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5725 - accuracy: 0.8338 - val_loss: 1.9448 - val_accuracy: 0.4690\n",
      "Epoch 631/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5618 - accuracy: 0.8338 - val_loss: 1.7962 - val_accuracy: 0.5044\n",
      "Epoch 632/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5519 - accuracy: 0.8368 - val_loss: 1.8388 - val_accuracy: 0.4425\n",
      "Epoch 633/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5662 - accuracy: 0.8338 - val_loss: 1.7999 - val_accuracy: 0.4602\n",
      "Epoch 634/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5563 - accuracy: 0.8368 - val_loss: 1.7108 - val_accuracy: 0.4690\n",
      "Epoch 635/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5422 - accuracy: 0.8398 - val_loss: 1.6316 - val_accuracy: 0.5310\n",
      "Epoch 636/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5362 - accuracy: 0.8338 - val_loss: 1.6666 - val_accuracy: 0.5044\n",
      "Epoch 637/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5468 - accuracy: 0.8249 - val_loss: 1.8783 - val_accuracy: 0.3894\n",
      "Epoch 638/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5685 - accuracy: 0.8190 - val_loss: 1.9940 - val_accuracy: 0.4336\n",
      "Epoch 639/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6170 - accuracy: 0.8249 - val_loss: 1.9899 - val_accuracy: 0.4336\n",
      "Epoch 640/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6290 - accuracy: 0.8249 - val_loss: 1.9254 - val_accuracy: 0.4425\n",
      "Epoch 641/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6047 - accuracy: 0.8338 - val_loss: 1.7275 - val_accuracy: 0.4956\n",
      "Epoch 642/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6210 - accuracy: 0.8160 - val_loss: 1.9380 - val_accuracy: 0.4690\n",
      "Epoch 643/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6819 - accuracy: 0.8160 - val_loss: 1.9144 - val_accuracy: 0.4513\n",
      "Epoch 644/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5912 - accuracy: 0.8220 - val_loss: 1.7825 - val_accuracy: 0.4513\n",
      "Epoch 645/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5656 - accuracy: 0.8309 - val_loss: 1.8402 - val_accuracy: 0.4602\n",
      "Epoch 646/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5720 - accuracy: 0.8368 - val_loss: 1.7977 - val_accuracy: 0.4602\n",
      "Epoch 647/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5496 - accuracy: 0.8457 - val_loss: 1.8324 - val_accuracy: 0.5044\n",
      "Epoch 648/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5726 - accuracy: 0.8427 - val_loss: 33.9571 - val_accuracy: 0.2566\n",
      "Epoch 649/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5615 - accuracy: 0.8338 - val_loss: 1.8247 - val_accuracy: 0.4513\n",
      "Epoch 650/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5669 - accuracy: 0.8220 - val_loss: 1.8595 - val_accuracy: 0.4956\n",
      "Epoch 651/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5843 - accuracy: 0.8160 - val_loss: 1.7900 - val_accuracy: 0.5133\n",
      "Epoch 652/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5666 - accuracy: 0.8220 - val_loss: 1.7949 - val_accuracy: 0.5221\n",
      "Epoch 653/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5624 - accuracy: 0.8309 - val_loss: 1.7265 - val_accuracy: 0.5398\n",
      "Epoch 654/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5657 - accuracy: 0.8279 - val_loss: 1.9356 - val_accuracy: 0.5044\n",
      "Epoch 655/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5610 - accuracy: 0.8220 - val_loss: 2.0449 - val_accuracy: 0.4513\n",
      "Epoch 656/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5583 - accuracy: 0.8338 - val_loss: 1.9492 - val_accuracy: 0.4867\n",
      "Epoch 657/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5543 - accuracy: 0.8516 - val_loss: 1.7170 - val_accuracy: 0.5044\n",
      "Epoch 658/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5453 - accuracy: 0.8279 - val_loss: 1.8083 - val_accuracy: 0.5398\n",
      "Epoch 659/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5455 - accuracy: 0.8427 - val_loss: 1.8522 - val_accuracy: 0.5221\n",
      "Epoch 660/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5446 - accuracy: 0.8338 - val_loss: 1.9051 - val_accuracy: 0.4602\n",
      "Epoch 661/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5432 - accuracy: 0.8398 - val_loss: 1.9076 - val_accuracy: 0.4425\n",
      "Epoch 662/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5688 - accuracy: 0.8338 - val_loss: 1.9820 - val_accuracy: 0.4513\n",
      "Epoch 663/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5656 - accuracy: 0.8427 - val_loss: 2.5831 - val_accuracy: 0.3894\n",
      "Epoch 664/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6039 - accuracy: 0.8160 - val_loss: 2.0374 - val_accuracy: 0.4956\n",
      "Epoch 665/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5575 - accuracy: 0.8249 - val_loss: 1.7694 - val_accuracy: 0.4690\n",
      "Epoch 666/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5351 - accuracy: 0.8427 - val_loss: 1.7995 - val_accuracy: 0.5221\n",
      "Epoch 667/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5276 - accuracy: 0.8368 - val_loss: 1.7423 - val_accuracy: 0.4956\n",
      "Epoch 668/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5292 - accuracy: 0.8487 - val_loss: 1.7318 - val_accuracy: 0.5044\n",
      "Epoch 669/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5285 - accuracy: 0.8516 - val_loss: 1.7490 - val_accuracy: 0.5044\n",
      "Epoch 670/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5238 - accuracy: 0.8368 - val_loss: 1.8987 - val_accuracy: 0.5221\n",
      "Epoch 671/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5303 - accuracy: 0.8368 - val_loss: 1.7877 - val_accuracy: 0.5133\n",
      "Epoch 672/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5280 - accuracy: 0.8487 - val_loss: 1.7640 - val_accuracy: 0.5133\n",
      "Epoch 673/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5398 - accuracy: 0.8398 - val_loss: 1.9759 - val_accuracy: 0.3982\n",
      "Epoch 674/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5331 - accuracy: 0.8368 - val_loss: 1.9652 - val_accuracy: 0.4425\n",
      "Epoch 675/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5225 - accuracy: 0.8368 - val_loss: 1.9955 - val_accuracy: 0.4867\n",
      "Epoch 676/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5305 - accuracy: 0.8457 - val_loss: 1.8277 - val_accuracy: 0.4779\n",
      "Epoch 677/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5327 - accuracy: 0.8605 - val_loss: 1.9394 - val_accuracy: 0.5044\n",
      "Epoch 678/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5326 - accuracy: 0.8427 - val_loss: 2.0036 - val_accuracy: 0.4425\n",
      "Epoch 679/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5310 - accuracy: 0.8368 - val_loss: 1.8708 - val_accuracy: 0.4690\n",
      "Epoch 680/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5158 - accuracy: 0.8487 - val_loss: 1.8491 - val_accuracy: 0.4956\n",
      "Epoch 681/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4997 - accuracy: 0.8546 - val_loss: 1.8790 - val_accuracy: 0.4867\n",
      "Epoch 682/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5215 - accuracy: 0.8398 - val_loss: 1.9302 - val_accuracy: 0.4602\n",
      "Epoch 683/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5789 - accuracy: 0.8398 - val_loss: 1.9751 - val_accuracy: 0.4779\n",
      "Epoch 684/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5152 - accuracy: 0.8576 - val_loss: 1.9856 - val_accuracy: 0.5044\n",
      "Epoch 685/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5119 - accuracy: 0.8635 - val_loss: 2.0051 - val_accuracy: 0.4867\n",
      "Epoch 686/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5130 - accuracy: 0.8457 - val_loss: 1.9005 - val_accuracy: 0.5221\n",
      "Epoch 687/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4924 - accuracy: 0.8576 - val_loss: 1.8111 - val_accuracy: 0.4867\n",
      "Epoch 688/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4747 - accuracy: 0.8635 - val_loss: 1.8666 - val_accuracy: 0.4956\n",
      "Epoch 689/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4675 - accuracy: 0.8635 - val_loss: 1.6858 - val_accuracy: 0.5044\n",
      "Epoch 690/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4804 - accuracy: 0.8576 - val_loss: 1.7279 - val_accuracy: 0.5221\n",
      "Epoch 691/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5195 - accuracy: 0.8546 - val_loss: 1.8231 - val_accuracy: 0.4602\n",
      "Epoch 692/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5143 - accuracy: 0.8546 - val_loss: 1.7186 - val_accuracy: 0.4867\n",
      "Epoch 693/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4542 - accuracy: 0.8724 - val_loss: 2.1498 - val_accuracy: 0.4513\n",
      "Epoch 694/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4930 - accuracy: 0.8516 - val_loss: 1.8124 - val_accuracy: 0.5044\n",
      "Epoch 695/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5677 - accuracy: 0.8160 - val_loss: 1.9306 - val_accuracy: 0.4956\n",
      "Epoch 696/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.6133 - accuracy: 0.8190 - val_loss: 2.1209 - val_accuracy: 0.5044\n",
      "Epoch 697/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5630 - accuracy: 0.8279 - val_loss: 1.9575 - val_accuracy: 0.4690\n",
      "Epoch 698/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5105 - accuracy: 0.8398 - val_loss: 2.0225 - val_accuracy: 0.5044\n",
      "Epoch 699/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5652 - accuracy: 0.8398 - val_loss: 2.0466 - val_accuracy: 0.4690\n",
      "Epoch 700/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5119 - accuracy: 0.8457 - val_loss: 2.1268 - val_accuracy: 0.5133\n",
      "Epoch 701/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5485 - accuracy: 0.8427 - val_loss: 1.9100 - val_accuracy: 0.5310\n",
      "Epoch 702/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5524 - accuracy: 0.8487 - val_loss: 1.8232 - val_accuracy: 0.5044\n",
      "Epoch 703/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5323 - accuracy: 0.8427 - val_loss: 1.8586 - val_accuracy: 0.4690\n",
      "Epoch 704/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5331 - accuracy: 0.8368 - val_loss: 1.8846 - val_accuracy: 0.4867\n",
      "Epoch 705/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5116 - accuracy: 0.8487 - val_loss: 1.8892 - val_accuracy: 0.4248\n",
      "Epoch 706/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4810 - accuracy: 0.8487 - val_loss: 1.8824 - val_accuracy: 0.4690\n",
      "Epoch 707/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4843 - accuracy: 0.8576 - val_loss: 1.7287 - val_accuracy: 0.4690\n",
      "Epoch 708/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4910 - accuracy: 0.8398 - val_loss: 1.6069 - val_accuracy: 0.5310\n",
      "Epoch 709/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4729 - accuracy: 0.8516 - val_loss: 1.7140 - val_accuracy: 0.5044\n",
      "Epoch 710/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4588 - accuracy: 0.8576 - val_loss: 1.6483 - val_accuracy: 0.5133\n",
      "Epoch 711/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4900 - accuracy: 0.8457 - val_loss: 1.8421 - val_accuracy: 0.4690\n",
      "Epoch 712/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5186 - accuracy: 0.8398 - val_loss: 1.7882 - val_accuracy: 0.5133\n",
      "Epoch 713/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4815 - accuracy: 0.8546 - val_loss: 1.7796 - val_accuracy: 0.4956\n",
      "Epoch 714/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4823 - accuracy: 0.8546 - val_loss: 1.7022 - val_accuracy: 0.5221\n",
      "Epoch 715/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4707 - accuracy: 0.8576 - val_loss: 1.8036 - val_accuracy: 0.5133\n",
      "Epoch 716/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4996 - accuracy: 0.8546 - val_loss: 1.8809 - val_accuracy: 0.4956\n",
      "Epoch 717/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4658 - accuracy: 0.8635 - val_loss: 2.1322 - val_accuracy: 0.4956\n",
      "Epoch 718/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5188 - accuracy: 0.8398 - val_loss: 1.9271 - val_accuracy: 0.4690\n",
      "Epoch 719/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5075 - accuracy: 0.8516 - val_loss: 1.7867 - val_accuracy: 0.5044\n",
      "Epoch 720/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5351 - accuracy: 0.8338 - val_loss: 1.8962 - val_accuracy: 0.5221\n",
      "Epoch 721/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5191 - accuracy: 0.8338 - val_loss: 2.1387 - val_accuracy: 0.4513\n",
      "Epoch 722/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5238 - accuracy: 0.8309 - val_loss: 2.0444 - val_accuracy: 0.5221\n",
      "Epoch 723/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5187 - accuracy: 0.8338 - val_loss: 1.8434 - val_accuracy: 0.4867\n",
      "Epoch 724/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4939 - accuracy: 0.8487 - val_loss: 1.8620 - val_accuracy: 0.5044\n",
      "Epoch 725/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4906 - accuracy: 0.8487 - val_loss: 1.7833 - val_accuracy: 0.4690\n",
      "Epoch 726/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4934 - accuracy: 0.8546 - val_loss: 1.7651 - val_accuracy: 0.5133\n",
      "Epoch 727/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4832 - accuracy: 0.8576 - val_loss: 1.7705 - val_accuracy: 0.5310\n",
      "Epoch 728/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4807 - accuracy: 0.8546 - val_loss: 1.7283 - val_accuracy: 0.5044\n",
      "Epoch 729/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4839 - accuracy: 0.8576 - val_loss: 1.9501 - val_accuracy: 0.4867\n",
      "Epoch 730/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4800 - accuracy: 0.8487 - val_loss: 1.9595 - val_accuracy: 0.5044\n",
      "Epoch 731/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4971 - accuracy: 0.8427 - val_loss: 1.8745 - val_accuracy: 0.4779\n",
      "Epoch 732/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4941 - accuracy: 0.8457 - val_loss: 1.8383 - val_accuracy: 0.5133\n",
      "Epoch 733/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5002 - accuracy: 0.8487 - val_loss: 1.8120 - val_accuracy: 0.4867\n",
      "Epoch 734/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4977 - accuracy: 0.8546 - val_loss: 1.7655 - val_accuracy: 0.4690\n",
      "Epoch 735/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5266 - accuracy: 0.8427 - val_loss: 1.9389 - val_accuracy: 0.5044\n",
      "Epoch 736/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5376 - accuracy: 0.8368 - val_loss: 2.0949 - val_accuracy: 0.4690\n",
      "Epoch 737/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5546 - accuracy: 0.8338 - val_loss: 1.9644 - val_accuracy: 0.4867\n",
      "Epoch 738/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5770 - accuracy: 0.8190 - val_loss: 2.0080 - val_accuracy: 0.4867\n",
      "Epoch 739/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5827 - accuracy: 0.8190 - val_loss: 1.9299 - val_accuracy: 0.5221\n",
      "Epoch 740/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5687 - accuracy: 0.8220 - val_loss: 1.8897 - val_accuracy: 0.5044\n",
      "Epoch 741/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5392 - accuracy: 0.8398 - val_loss: 1.8395 - val_accuracy: 0.4956\n",
      "Epoch 742/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5190 - accuracy: 0.8398 - val_loss: 1.8089 - val_accuracy: 0.4779\n",
      "Epoch 743/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5235 - accuracy: 0.8398 - val_loss: 1.8524 - val_accuracy: 0.4779\n",
      "Epoch 744/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5239 - accuracy: 0.8427 - val_loss: 2.0078 - val_accuracy: 0.4956\n",
      "Epoch 745/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5007 - accuracy: 0.8487 - val_loss: 1.9898 - val_accuracy: 0.4690\n",
      "Epoch 746/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5344 - accuracy: 0.8427 - val_loss: 1.9468 - val_accuracy: 0.4690\n",
      "Epoch 747/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5100 - accuracy: 0.8457 - val_loss: 2.0278 - val_accuracy: 0.4867\n",
      "Epoch 748/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4855 - accuracy: 0.8457 - val_loss: 1.9898 - val_accuracy: 0.4956\n",
      "Epoch 749/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4939 - accuracy: 0.8546 - val_loss: 1.9575 - val_accuracy: 0.4690\n",
      "Epoch 750/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4890 - accuracy: 0.8546 - val_loss: 1.9899 - val_accuracy: 0.4779\n",
      "Epoch 751/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5057 - accuracy: 0.8368 - val_loss: 1.9776 - val_accuracy: 0.4867\n",
      "Epoch 752/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5311 - accuracy: 0.8338 - val_loss: 2.1866 - val_accuracy: 0.3717\n",
      "Epoch 753/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5313 - accuracy: 0.8487 - val_loss: 2.0026 - val_accuracy: 0.4159\n",
      "Epoch 754/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5144 - accuracy: 0.8457 - val_loss: 1.9816 - val_accuracy: 0.4867\n",
      "Epoch 755/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4920 - accuracy: 0.8605 - val_loss: 1.9668 - val_accuracy: 0.3717\n",
      "Epoch 756/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5138 - accuracy: 0.8546 - val_loss: 2.0399 - val_accuracy: 0.5044\n",
      "Epoch 757/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.4742 - accuracy: 0.8576 - val_loss: 1.9952 - val_accuracy: 0.4602\n",
      "Epoch 758/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5010 - accuracy: 0.8516 - val_loss: 2.1422 - val_accuracy: 0.4690\n",
      "Epoch 759/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5160 - accuracy: 0.8457 - val_loss: 2.0880 - val_accuracy: 0.4690\n",
      "Epoch 760/1000\n",
      "11/11 [==============================] - 39s 4s/step - loss: 0.5207 - accuracy: 0.8546 - val_loss: 1.9973 - val_accuracy: 0.5044\n",
      "Epoch 761/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4773 - accuracy: 0.8665 - val_loss: 1.8691 - val_accuracy: 0.4867\n",
      "Epoch 762/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4712 - accuracy: 0.8665 - val_loss: 1.8845 - val_accuracy: 0.4956\n",
      "Epoch 763/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5963 - accuracy: 0.8249 - val_loss: 1.9750 - val_accuracy: 0.4867\n",
      "Epoch 764/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4882 - accuracy: 0.8516 - val_loss: 1.9297 - val_accuracy: 0.4956\n",
      "Epoch 765/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4894 - accuracy: 0.8605 - val_loss: 2.0657 - val_accuracy: 0.4602\n",
      "Epoch 766/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5456 - accuracy: 0.8279 - val_loss: 2.1350 - val_accuracy: 0.4513\n",
      "Epoch 767/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5639 - accuracy: 0.8249 - val_loss: 2.1318 - val_accuracy: 0.4690\n",
      "Epoch 768/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5798 - accuracy: 0.8338 - val_loss: 2.0843 - val_accuracy: 0.5044\n",
      "Epoch 769/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5241 - accuracy: 0.8368 - val_loss: 2.0938 - val_accuracy: 0.5221\n",
      "Epoch 770/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.5040 - accuracy: 0.8546 - val_loss: 2.0920 - val_accuracy: 0.4336\n",
      "Epoch 771/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4892 - accuracy: 0.8635 - val_loss: 1.8886 - val_accuracy: 0.4956\n",
      "Epoch 772/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4856 - accuracy: 0.8546 - val_loss: 1.9042 - val_accuracy: 0.4513\n",
      "Epoch 773/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4948 - accuracy: 0.8457 - val_loss: 2.1085 - val_accuracy: 0.4602\n",
      "Epoch 774/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4734 - accuracy: 0.8576 - val_loss: 1.9330 - val_accuracy: 0.4779\n",
      "Epoch 775/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4461 - accuracy: 0.8635 - val_loss: 1.7972 - val_accuracy: 0.4956\n",
      "Epoch 776/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4605 - accuracy: 0.8605 - val_loss: 1.9139 - val_accuracy: 0.5044\n",
      "Epoch 777/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4652 - accuracy: 0.8576 - val_loss: 1.9092 - val_accuracy: 0.5133\n",
      "Epoch 778/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4776 - accuracy: 0.8487 - val_loss: 1.9486 - val_accuracy: 0.4956\n",
      "Epoch 779/1000\n",
      "11/11 [==============================] - 46s 4s/step - loss: 0.5326 - accuracy: 0.8338 - val_loss: 1.9346 - val_accuracy: 0.4956\n",
      "Epoch 780/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.5005 - accuracy: 0.8398 - val_loss: 1.9331 - val_accuracy: 0.4779\n",
      "Epoch 781/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.4976 - accuracy: 0.8457 - val_loss: 1.8352 - val_accuracy: 0.5133\n",
      "Epoch 782/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5095 - accuracy: 0.8457 - val_loss: 1.9414 - val_accuracy: 0.5133\n",
      "Epoch 783/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4960 - accuracy: 0.8457 - val_loss: 1.8669 - val_accuracy: 0.4956\n",
      "Epoch 784/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.5575 - accuracy: 0.8338 - val_loss: 1.8783 - val_accuracy: 0.4956\n",
      "Epoch 785/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.4744 - accuracy: 0.8546 - val_loss: 2.0040 - val_accuracy: 0.4690\n",
      "Epoch 786/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.5129 - accuracy: 0.8309 - val_loss: 1.8507 - val_accuracy: 0.4779\n",
      "Epoch 787/1000\n",
      "11/11 [==============================] - 46s 4s/step - loss: 0.5058 - accuracy: 0.8398 - val_loss: 2.0977 - val_accuracy: 0.4513\n",
      "Epoch 788/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.4983 - accuracy: 0.8457 - val_loss: 1.9909 - val_accuracy: 0.4071\n",
      "Epoch 789/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.4780 - accuracy: 0.8576 - val_loss: 1.9647 - val_accuracy: 0.4071\n",
      "Epoch 790/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.4718 - accuracy: 0.8635 - val_loss: 2.1003 - val_accuracy: 0.4602\n",
      "Epoch 791/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.4458 - accuracy: 0.8694 - val_loss: 2.1113 - val_accuracy: 0.4602\n",
      "Epoch 792/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.4513 - accuracy: 0.8635 - val_loss: 1.9393 - val_accuracy: 0.4248\n",
      "Epoch 793/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.4711 - accuracy: 0.8694 - val_loss: 1.9842 - val_accuracy: 0.4602\n",
      "Epoch 794/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4812 - accuracy: 0.8605 - val_loss: 2.0505 - val_accuracy: 0.4602\n",
      "Epoch 795/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.4467 - accuracy: 0.8694 - val_loss: 1.8906 - val_accuracy: 0.5133\n",
      "Epoch 796/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4471 - accuracy: 0.8605 - val_loss: 1.8935 - val_accuracy: 0.4867\n",
      "Epoch 797/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4575 - accuracy: 0.8605 - val_loss: 1.9550 - val_accuracy: 0.5133\n",
      "Epoch 798/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4728 - accuracy: 0.8457 - val_loss: 1.9594 - val_accuracy: 0.4690\n",
      "Epoch 799/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4909 - accuracy: 0.8487 - val_loss: 2.0013 - val_accuracy: 0.4513\n",
      "Epoch 800/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4904 - accuracy: 0.8605 - val_loss: 2.1452 - val_accuracy: 0.4602\n",
      "Epoch 801/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4648 - accuracy: 0.8635 - val_loss: 2.0515 - val_accuracy: 0.4513\n",
      "Epoch 802/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4644 - accuracy: 0.8694 - val_loss: 1.9594 - val_accuracy: 0.4956\n",
      "Epoch 803/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4887 - accuracy: 0.8665 - val_loss: 2.2015 - val_accuracy: 0.4779\n",
      "Epoch 804/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4693 - accuracy: 0.8694 - val_loss: 2.1271 - val_accuracy: 0.4867\n",
      "Epoch 805/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4737 - accuracy: 0.8665 - val_loss: 2.1955 - val_accuracy: 0.4690\n",
      "Epoch 806/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4580 - accuracy: 0.8694 - val_loss: 2.1524 - val_accuracy: 0.4867\n",
      "Epoch 807/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.4353 - accuracy: 0.8813 - val_loss: 2.1150 - val_accuracy: 0.4779\n",
      "Epoch 808/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.4372 - accuracy: 0.8754 - val_loss: 2.1849 - val_accuracy: 0.4779\n",
      "Epoch 809/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4425 - accuracy: 0.8635 - val_loss: 2.1926 - val_accuracy: 0.4425\n",
      "Epoch 810/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.4359 - accuracy: 0.8754 - val_loss: 2.1864 - val_accuracy: 0.4425\n",
      "Epoch 811/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4170 - accuracy: 0.8843 - val_loss: 2.1558 - val_accuracy: 0.4867\n",
      "Epoch 812/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.4619 - accuracy: 0.8487 - val_loss: 2.2619 - val_accuracy: 0.3628\n",
      "Epoch 813/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4487 - accuracy: 0.8724 - val_loss: 2.3158 - val_accuracy: 0.4513\n",
      "Epoch 814/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4409 - accuracy: 0.8665 - val_loss: 2.1824 - val_accuracy: 0.4956\n",
      "Epoch 815/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4135 - accuracy: 0.8813 - val_loss: 1.9942 - val_accuracy: 0.4602\n",
      "Epoch 816/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.4887 - accuracy: 0.8576 - val_loss: 2.2658 - val_accuracy: 0.4513\n",
      "Epoch 817/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.4879 - accuracy: 0.8576 - val_loss: 2.0392 - val_accuracy: 0.4425\n",
      "Epoch 818/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5015 - accuracy: 0.8516 - val_loss: 2.3339 - val_accuracy: 0.4513\n",
      "Epoch 819/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4825 - accuracy: 0.8576 - val_loss: 2.3329 - val_accuracy: 0.4425\n",
      "Epoch 820/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4814 - accuracy: 0.8576 - val_loss: 2.2604 - val_accuracy: 0.4159\n",
      "Epoch 821/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4833 - accuracy: 0.8665 - val_loss: 1.9419 - val_accuracy: 0.4779\n",
      "Epoch 822/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5078 - accuracy: 0.8398 - val_loss: 1.9488 - val_accuracy: 0.4867\n",
      "Epoch 823/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5003 - accuracy: 0.8546 - val_loss: 1.9634 - val_accuracy: 0.4690\n",
      "Epoch 824/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4621 - accuracy: 0.8546 - val_loss: 1.9318 - val_accuracy: 0.4779\n",
      "Epoch 825/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4571 - accuracy: 0.8665 - val_loss: 1.9042 - val_accuracy: 0.4867\n",
      "Epoch 826/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.4390 - accuracy: 0.8665 - val_loss: 1.9343 - val_accuracy: 0.4779\n",
      "Epoch 827/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4539 - accuracy: 0.8665 - val_loss: 1.9991 - val_accuracy: 0.5221\n",
      "Epoch 828/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4131 - accuracy: 0.8843 - val_loss: 1.8358 - val_accuracy: 0.4956\n",
      "Epoch 829/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3961 - accuracy: 0.8932 - val_loss: 1.9164 - val_accuracy: 0.4336\n",
      "Epoch 830/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4071 - accuracy: 0.8843 - val_loss: 2.0500 - val_accuracy: 0.4779\n",
      "Epoch 831/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3982 - accuracy: 0.8872 - val_loss: 1.9526 - val_accuracy: 0.4690\n",
      "Epoch 832/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4275 - accuracy: 0.8902 - val_loss: 2.6904 - val_accuracy: 0.2301\n",
      "Epoch 833/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4145 - accuracy: 0.8754 - val_loss: 1.8800 - val_accuracy: 0.4336\n",
      "Epoch 834/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4292 - accuracy: 0.8783 - val_loss: 2.4594 - val_accuracy: 0.2655\n",
      "Epoch 835/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4855 - accuracy: 0.8605 - val_loss: 2.1292 - val_accuracy: 0.4159\n",
      "Epoch 836/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4297 - accuracy: 0.8813 - val_loss: 1.9769 - val_accuracy: 0.4779\n",
      "Epoch 837/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4224 - accuracy: 0.8694 - val_loss: 2.0764 - val_accuracy: 0.4513\n",
      "Epoch 838/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4109 - accuracy: 0.8902 - val_loss: 2.0263 - val_accuracy: 0.4425\n",
      "Epoch 839/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4076 - accuracy: 0.8902 - val_loss: 2.0585 - val_accuracy: 0.3717\n",
      "Epoch 840/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4173 - accuracy: 0.8724 - val_loss: 2.1145 - val_accuracy: 0.4425\n",
      "Epoch 841/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4073 - accuracy: 0.8813 - val_loss: 2.1812 - val_accuracy: 0.4513\n",
      "Epoch 842/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4155 - accuracy: 0.8813 - val_loss: 2.2293 - val_accuracy: 0.4336\n",
      "Epoch 843/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4031 - accuracy: 0.8843 - val_loss: 2.0962 - val_accuracy: 0.4336\n",
      "Epoch 844/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3901 - accuracy: 0.8961 - val_loss: 2.1929 - val_accuracy: 0.4779\n",
      "Epoch 845/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4187 - accuracy: 0.8813 - val_loss: 2.2109 - val_accuracy: 0.4690\n",
      "Epoch 846/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4193 - accuracy: 0.8783 - val_loss: 2.1548 - val_accuracy: 0.5044\n",
      "Epoch 847/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4024 - accuracy: 0.8783 - val_loss: 2.1569 - val_accuracy: 0.4425\n",
      "Epoch 848/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4017 - accuracy: 0.8754 - val_loss: 2.1159 - val_accuracy: 0.4425\n",
      "Epoch 849/1000\n",
      "11/11 [==============================] - 40s 4s/step - loss: 0.4355 - accuracy: 0.8872 - val_loss: 2.1022 - val_accuracy: 0.4779\n",
      "Epoch 850/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4050 - accuracy: 0.8872 - val_loss: 1.9965 - val_accuracy: 0.4690\n",
      "Epoch 851/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3992 - accuracy: 0.8843 - val_loss: 2.1111 - val_accuracy: 0.4867\n",
      "Epoch 852/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4086 - accuracy: 0.8605 - val_loss: 2.1886 - val_accuracy: 0.4425\n",
      "Epoch 853/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5531 - accuracy: 0.8279 - val_loss: 2.4379 - val_accuracy: 0.4071\n",
      "Epoch 854/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4778 - accuracy: 0.8546 - val_loss: 2.1749 - val_accuracy: 0.4248\n",
      "Epoch 855/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4868 - accuracy: 0.8635 - val_loss: 2.8368 - val_accuracy: 0.4336\n",
      "Epoch 856/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4924 - accuracy: 0.8665 - val_loss: 2.2357 - val_accuracy: 0.3982\n",
      "Epoch 857/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4900 - accuracy: 0.8665 - val_loss: 2.0556 - val_accuracy: 0.4690\n",
      "Epoch 858/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4751 - accuracy: 0.8724 - val_loss: 1.8952 - val_accuracy: 0.5133\n",
      "Epoch 859/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4506 - accuracy: 0.8754 - val_loss: 1.8961 - val_accuracy: 0.4690\n",
      "Epoch 860/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4520 - accuracy: 0.8754 - val_loss: 1.9085 - val_accuracy: 0.5044\n",
      "Epoch 861/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4707 - accuracy: 0.8635 - val_loss: 2.0948 - val_accuracy: 0.4602\n",
      "Epoch 862/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.6905 - accuracy: 0.8042 - val_loss: 1.9138 - val_accuracy: 0.4867\n",
      "Epoch 863/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5145 - accuracy: 0.8457 - val_loss: 2.0324 - val_accuracy: 0.4779\n",
      "Epoch 864/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.5028 - accuracy: 0.8427 - val_loss: 1.8935 - val_accuracy: 0.5133\n",
      "Epoch 865/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4604 - accuracy: 0.8605 - val_loss: 1.8810 - val_accuracy: 0.4779\n",
      "Epoch 866/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4543 - accuracy: 0.8694 - val_loss: 1.9242 - val_accuracy: 0.4336\n",
      "Epoch 867/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4481 - accuracy: 0.8694 - val_loss: 2.5305 - val_accuracy: 0.2832\n",
      "Epoch 868/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4680 - accuracy: 0.8576 - val_loss: 1.9487 - val_accuracy: 0.5221\n",
      "Epoch 869/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4735 - accuracy: 0.8694 - val_loss: 2.2045 - val_accuracy: 0.4602\n",
      "Epoch 870/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4434 - accuracy: 0.8665 - val_loss: 3.0534 - val_accuracy: 0.2743\n",
      "Epoch 871/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4431 - accuracy: 0.8724 - val_loss: 2.8064 - val_accuracy: 0.2566\n",
      "Epoch 872/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4787 - accuracy: 0.8427 - val_loss: 2.8186 - val_accuracy: 0.3363\n",
      "Epoch 873/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4159 - accuracy: 0.8665 - val_loss: 2.1622 - val_accuracy: 0.4602\n",
      "Epoch 874/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3951 - accuracy: 0.8902 - val_loss: 2.1075 - val_accuracy: 0.4602\n",
      "Epoch 875/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4211 - accuracy: 0.8724 - val_loss: 2.6434 - val_accuracy: 0.3274\n",
      "Epoch 876/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3989 - accuracy: 0.8783 - val_loss: 2.1097 - val_accuracy: 0.4159\n",
      "Epoch 877/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3988 - accuracy: 0.8932 - val_loss: 1.9560 - val_accuracy: 0.4779\n",
      "Epoch 878/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4080 - accuracy: 0.8843 - val_loss: 2.0305 - val_accuracy: 0.4602\n",
      "Epoch 879/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4081 - accuracy: 0.8754 - val_loss: 2.1441 - val_accuracy: 0.4159\n",
      "Epoch 880/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4380 - accuracy: 0.8724 - val_loss: 2.9344 - val_accuracy: 0.2743\n",
      "Epoch 881/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4520 - accuracy: 0.8724 - val_loss: 2.5057 - val_accuracy: 0.2566\n",
      "Epoch 882/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4225 - accuracy: 0.8872 - val_loss: 2.3367 - val_accuracy: 0.3717\n",
      "Epoch 883/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4340 - accuracy: 0.8783 - val_loss: 1.9435 - val_accuracy: 0.4690\n",
      "Epoch 884/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4380 - accuracy: 0.8665 - val_loss: 1.9558 - val_accuracy: 0.5044\n",
      "Epoch 885/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4669 - accuracy: 0.8754 - val_loss: 2.4347 - val_accuracy: 0.4513\n",
      "Epoch 886/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4420 - accuracy: 0.8754 - val_loss: 2.2887 - val_accuracy: 0.4956\n",
      "Epoch 887/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4768 - accuracy: 0.8605 - val_loss: 2.2083 - val_accuracy: 0.4513\n",
      "Epoch 888/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4628 - accuracy: 0.8665 - val_loss: 2.2508 - val_accuracy: 0.4867\n",
      "Epoch 889/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4400 - accuracy: 0.8783 - val_loss: 2.1577 - val_accuracy: 0.4779\n",
      "Epoch 890/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4417 - accuracy: 0.8813 - val_loss: 2.2151 - val_accuracy: 0.4159\n",
      "Epoch 891/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4438 - accuracy: 0.8843 - val_loss: 2.1414 - val_accuracy: 0.4336\n",
      "Epoch 892/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4298 - accuracy: 0.8813 - val_loss: 2.1689 - val_accuracy: 0.4159\n",
      "Epoch 893/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4260 - accuracy: 0.8694 - val_loss: 2.0289 - val_accuracy: 0.4867\n",
      "Epoch 894/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4080 - accuracy: 0.8694 - val_loss: 1.7169 - val_accuracy: 0.4690\n",
      "Epoch 895/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3967 - accuracy: 0.8813 - val_loss: 1.7862 - val_accuracy: 0.4425\n",
      "Epoch 896/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3868 - accuracy: 0.8872 - val_loss: 1.8327 - val_accuracy: 0.4602\n",
      "Epoch 897/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3828 - accuracy: 0.8872 - val_loss: 1.9064 - val_accuracy: 0.4867\n",
      "Epoch 898/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3757 - accuracy: 0.8932 - val_loss: 1.7587 - val_accuracy: 0.4956\n",
      "Epoch 899/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3789 - accuracy: 0.8961 - val_loss: 1.9578 - val_accuracy: 0.4690\n",
      "Epoch 900/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3762 - accuracy: 0.8961 - val_loss: 1.9227 - val_accuracy: 0.5044\n",
      "Epoch 901/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3770 - accuracy: 0.8843 - val_loss: 1.8845 - val_accuracy: 0.4336\n",
      "Epoch 902/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3705 - accuracy: 0.9021 - val_loss: 1.8940 - val_accuracy: 0.4956\n",
      "Epoch 903/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3676 - accuracy: 0.8902 - val_loss: 2.0137 - val_accuracy: 0.4248\n",
      "Epoch 904/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3790 - accuracy: 0.8932 - val_loss: 2.2407 - val_accuracy: 0.4248\n",
      "Epoch 905/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3681 - accuracy: 0.8872 - val_loss: 2.2692 - val_accuracy: 0.4602\n",
      "Epoch 906/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3897 - accuracy: 0.8902 - val_loss: 2.4963 - val_accuracy: 0.4248\n",
      "Epoch 907/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.4144 - accuracy: 0.8724 - val_loss: 2.3155 - val_accuracy: 0.4071\n",
      "Epoch 908/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3893 - accuracy: 0.8843 - val_loss: 2.2561 - val_accuracy: 0.4071\n",
      "Epoch 909/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3880 - accuracy: 0.8991 - val_loss: 2.2079 - val_accuracy: 0.4513\n",
      "Epoch 910/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3865 - accuracy: 0.9021 - val_loss: 2.1997 - val_accuracy: 0.4159\n",
      "Epoch 911/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3547 - accuracy: 0.8991 - val_loss: 2.0605 - val_accuracy: 0.4779\n",
      "Epoch 912/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3498 - accuracy: 0.9050 - val_loss: 2.6283 - val_accuracy: 0.3274\n",
      "Epoch 913/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3759 - accuracy: 0.8843 - val_loss: 2.1266 - val_accuracy: 0.4071\n",
      "Epoch 914/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3729 - accuracy: 0.8961 - val_loss: 2.2691 - val_accuracy: 0.4248\n",
      "Epoch 915/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3648 - accuracy: 0.8932 - val_loss: 3.1432 - val_accuracy: 0.2212\n",
      "Epoch 916/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3840 - accuracy: 0.8961 - val_loss: 3.6616 - val_accuracy: 0.1770\n",
      "Epoch 917/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3811 - accuracy: 0.8932 - val_loss: 3.2544 - val_accuracy: 0.1593\n",
      "Epoch 918/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3942 - accuracy: 0.8783 - val_loss: 1.9727 - val_accuracy: 0.5221\n",
      "Epoch 919/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3875 - accuracy: 0.8902 - val_loss: 1.9738 - val_accuracy: 0.4602\n",
      "Epoch 920/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3650 - accuracy: 0.8991 - val_loss: 2.0707 - val_accuracy: 0.4779\n",
      "Epoch 921/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3617 - accuracy: 0.8932 - val_loss: 2.0172 - val_accuracy: 0.4425\n",
      "Epoch 922/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3565 - accuracy: 0.9050 - val_loss: 2.8288 - val_accuracy: 0.2566\n",
      "Epoch 923/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3872 - accuracy: 0.8872 - val_loss: 1.9493 - val_accuracy: 0.5133\n",
      "Epoch 924/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3471 - accuracy: 0.8932 - val_loss: 2.1592 - val_accuracy: 0.4159\n",
      "Epoch 925/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3613 - accuracy: 0.9080 - val_loss: 2.2307 - val_accuracy: 0.4513\n",
      "Epoch 926/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3841 - accuracy: 0.9021 - val_loss: 2.1697 - val_accuracy: 0.4867\n",
      "Epoch 927/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3887 - accuracy: 0.8932 - val_loss: 1.9803 - val_accuracy: 0.4867\n",
      "Epoch 928/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3663 - accuracy: 0.8932 - val_loss: 2.1276 - val_accuracy: 0.4071\n",
      "Epoch 929/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3693 - accuracy: 0.8872 - val_loss: 2.7982 - val_accuracy: 0.3805\n",
      "Epoch 930/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3735 - accuracy: 0.8932 - val_loss: 2.2134 - val_accuracy: 0.4690\n",
      "Epoch 931/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3696 - accuracy: 0.8932 - val_loss: 3.5724 - val_accuracy: 0.2478\n",
      "Epoch 932/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3888 - accuracy: 0.8724 - val_loss: 2.9160 - val_accuracy: 0.3097\n",
      "Epoch 933/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3682 - accuracy: 0.8813 - val_loss: 3.0555 - val_accuracy: 0.2920\n",
      "Epoch 934/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3578 - accuracy: 0.8813 - val_loss: 2.4558 - val_accuracy: 0.4159\n",
      "Epoch 935/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3474 - accuracy: 0.8872 - val_loss: 1.9140 - val_accuracy: 0.4867\n",
      "Epoch 936/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3572 - accuracy: 0.8902 - val_loss: 2.0343 - val_accuracy: 0.4425\n",
      "Epoch 937/1000\n",
      "11/11 [==============================] - 49s 4s/step - loss: 0.3646 - accuracy: 0.9050 - val_loss: 2.1119 - val_accuracy: 0.4336\n",
      "Epoch 938/1000\n",
      "11/11 [==============================] - 47s 4s/step - loss: 0.3666 - accuracy: 0.8813 - val_loss: 2.0405 - val_accuracy: 0.4248\n",
      "Epoch 939/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.4008 - accuracy: 0.9021 - val_loss: 2.0313 - val_accuracy: 0.4779\n",
      "Epoch 940/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.4012 - accuracy: 0.8813 - val_loss: 1.9915 - val_accuracy: 0.4956\n",
      "Epoch 941/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3713 - accuracy: 0.8902 - val_loss: 2.0786 - val_accuracy: 0.4867\n",
      "Epoch 942/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3728 - accuracy: 0.8902 - val_loss: 2.1268 - val_accuracy: 0.4513\n",
      "Epoch 943/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3702 - accuracy: 0.8961 - val_loss: 2.3074 - val_accuracy: 0.4602\n",
      "Epoch 944/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3613 - accuracy: 0.9021 - val_loss: 2.3243 - val_accuracy: 0.4425\n",
      "Epoch 945/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.4223 - accuracy: 0.8783 - val_loss: 2.4615 - val_accuracy: 0.4602\n",
      "Epoch 946/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.4238 - accuracy: 0.8872 - val_loss: 2.4140 - val_accuracy: 0.4779\n",
      "Epoch 947/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.4100 - accuracy: 0.8724 - val_loss: 2.4017 - val_accuracy: 0.4690\n",
      "Epoch 948/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.4061 - accuracy: 0.8783 - val_loss: 2.4776 - val_accuracy: 0.4867\n",
      "Epoch 949/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3806 - accuracy: 0.8843 - val_loss: 2.3951 - val_accuracy: 0.4513\n",
      "Epoch 950/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3682 - accuracy: 0.8872 - val_loss: 2.3233 - val_accuracy: 0.4602\n",
      "Epoch 951/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3854 - accuracy: 0.8843 - val_loss: 2.2900 - val_accuracy: 0.4159\n",
      "Epoch 952/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3754 - accuracy: 0.8872 - val_loss: 2.2245 - val_accuracy: 0.4336\n",
      "Epoch 953/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3926 - accuracy: 0.8783 - val_loss: 2.2948 - val_accuracy: 0.4867\n",
      "Epoch 954/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3908 - accuracy: 0.8843 - val_loss: 2.4000 - val_accuracy: 0.4425\n",
      "Epoch 955/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3963 - accuracy: 0.8902 - val_loss: 2.2993 - val_accuracy: 0.4071\n",
      "Epoch 956/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3917 - accuracy: 0.8813 - val_loss: 2.3023 - val_accuracy: 0.4425\n",
      "Epoch 957/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3865 - accuracy: 0.8902 - val_loss: 2.2478 - val_accuracy: 0.4425\n",
      "Epoch 958/1000\n",
      "11/11 [==============================] - 54s 5s/step - loss: 0.3515 - accuracy: 0.9021 - val_loss: 2.1841 - val_accuracy: 0.4602\n",
      "Epoch 959/1000\n",
      "11/11 [==============================] - 47s 4s/step - loss: 0.3779 - accuracy: 0.8783 - val_loss: 2.1582 - val_accuracy: 0.4513\n",
      "Epoch 960/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3491 - accuracy: 0.8991 - val_loss: 2.2287 - val_accuracy: 0.5044\n",
      "Epoch 961/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3682 - accuracy: 0.8932 - val_loss: 2.3935 - val_accuracy: 0.5133\n",
      "Epoch 962/1000\n",
      "11/11 [==============================] - 46s 4s/step - loss: 0.3937 - accuracy: 0.8872 - val_loss: 2.5815 - val_accuracy: 0.4602\n",
      "Epoch 963/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3954 - accuracy: 0.8813 - val_loss: 2.6160 - val_accuracy: 0.4425\n",
      "Epoch 964/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3831 - accuracy: 0.8813 - val_loss: 2.6775 - val_accuracy: 0.3982\n",
      "Epoch 965/1000\n",
      "11/11 [==============================] - 49s 5s/step - loss: 0.3517 - accuracy: 0.9021 - val_loss: 2.7449 - val_accuracy: 0.3982\n",
      "Epoch 966/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3769 - accuracy: 0.8961 - val_loss: 2.5241 - val_accuracy: 0.3982\n",
      "Epoch 967/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3699 - accuracy: 0.8961 - val_loss: 2.3809 - val_accuracy: 0.4248\n",
      "Epoch 968/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3405 - accuracy: 0.9021 - val_loss: 2.3535 - val_accuracy: 0.4248\n",
      "Epoch 969/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3330 - accuracy: 0.9080 - val_loss: 2.4327 - val_accuracy: 0.4779\n",
      "Epoch 970/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3336 - accuracy: 0.9110 - val_loss: 2.5001 - val_accuracy: 0.4336\n",
      "Epoch 971/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3303 - accuracy: 0.9110 - val_loss: 2.2602 - val_accuracy: 0.4956\n",
      "Epoch 972/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3371 - accuracy: 0.9021 - val_loss: 2.3668 - val_accuracy: 0.4425\n",
      "Epoch 973/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3334 - accuracy: 0.9110 - val_loss: 2.5386 - val_accuracy: 0.4690\n",
      "Epoch 974/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3427 - accuracy: 0.9050 - val_loss: 2.4981 - val_accuracy: 0.3982\n",
      "Epoch 975/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.4014 - accuracy: 0.9021 - val_loss: 2.6392 - val_accuracy: 0.3274\n",
      "Epoch 976/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3356 - accuracy: 0.9199 - val_loss: 2.1556 - val_accuracy: 0.4159\n",
      "Epoch 977/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3383 - accuracy: 0.9139 - val_loss: 2.1203 - val_accuracy: 0.4336\n",
      "Epoch 978/1000\n",
      "11/11 [==============================] - 47s 4s/step - loss: 0.3622 - accuracy: 0.9021 - val_loss: 2.2365 - val_accuracy: 0.4867\n",
      "Epoch 979/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3691 - accuracy: 0.9050 - val_loss: 2.1017 - val_accuracy: 0.4425\n",
      "Epoch 980/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3329 - accuracy: 0.9139 - val_loss: 2.2010 - val_accuracy: 0.3982\n",
      "Epoch 981/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3381 - accuracy: 0.9110 - val_loss: 2.4404 - val_accuracy: 0.4248\n",
      "Epoch 982/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3288 - accuracy: 0.9050 - val_loss: 2.4041 - val_accuracy: 0.3982\n",
      "Epoch 983/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3324 - accuracy: 0.9050 - val_loss: 2.3715 - val_accuracy: 0.4071\n",
      "Epoch 984/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3309 - accuracy: 0.9050 - val_loss: 2.4769 - val_accuracy: 0.3805\n",
      "Epoch 985/1000\n",
      "11/11 [==============================] - 44s 4s/step - loss: 0.3550 - accuracy: 0.9021 - val_loss: 3.0585 - val_accuracy: 0.3982\n",
      "Epoch 986/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.3800 - accuracy: 0.8932 - val_loss: 3.1344 - val_accuracy: 0.3628\n",
      "Epoch 987/1000\n",
      "11/11 [==============================] - 51s 5s/step - loss: 0.3994 - accuracy: 0.8932 - val_loss: 3.2309 - val_accuracy: 0.3717\n",
      "Epoch 988/1000\n",
      "11/11 [==============================] - 50s 5s/step - loss: 0.4174 - accuracy: 0.8843 - val_loss: 3.1672 - val_accuracy: 0.3451\n",
      "Epoch 989/1000\n",
      "11/11 [==============================] - 51s 5s/step - loss: 0.4203 - accuracy: 0.8902 - val_loss: 2.7527 - val_accuracy: 0.3717\n",
      "Epoch 990/1000\n",
      "11/11 [==============================] - 50s 5s/step - loss: 0.4517 - accuracy: 0.8783 - val_loss: 2.7082 - val_accuracy: 0.3805\n",
      "Epoch 991/1000\n",
      "11/11 [==============================] - 45s 4s/step - loss: 0.4359 - accuracy: 0.8724 - val_loss: 2.6642 - val_accuracy: 0.4336\n",
      "Epoch 992/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3896 - accuracy: 0.8991 - val_loss: 2.4082 - val_accuracy: 0.4513\n",
      "Epoch 993/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3966 - accuracy: 0.8902 - val_loss: 2.4838 - val_accuracy: 0.4513\n",
      "Epoch 994/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3993 - accuracy: 0.8932 - val_loss: 2.5367 - val_accuracy: 0.4602\n",
      "Epoch 995/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3739 - accuracy: 0.9021 - val_loss: 2.4349 - val_accuracy: 0.4336\n",
      "Epoch 996/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3577 - accuracy: 0.9021 - val_loss: 2.2598 - val_accuracy: 0.4159\n",
      "Epoch 997/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3636 - accuracy: 0.9080 - val_loss: 2.3447 - val_accuracy: 0.4248\n",
      "Epoch 998/1000\n",
      "11/11 [==============================] - 43s 4s/step - loss: 0.3568 - accuracy: 0.8961 - val_loss: 2.3519 - val_accuracy: 0.4248\n",
      "Epoch 999/1000\n",
      "11/11 [==============================] - 42s 4s/step - loss: 0.3798 - accuracy: 0.8991 - val_loss: 2.4208 - val_accuracy: 0.4248\n",
      "Epoch 1000/1000\n",
      "11/11 [==============================] - 41s 4s/step - loss: 0.3595 - accuracy: 0.9080 - val_loss: 2.3673 - val_accuracy: 0.4425\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "history = cnn_model.fit(X_train, y_train_one_hot, epochs=1000, validation_data = (X_test, y_test_one_hot))\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classificadores import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 6s 577ms/step\n",
      "4/4 [==============================] - 2s 506ms/step\n"
     ]
    }
   ],
   "source": [
    "X_for_RF = feature_extractor.predict(X_train) #This is out X input to RF\n",
    "\n",
    "#RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_model = RandomForestClassifier(n_estimators = 500, random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "RF_model.fit(X_for_RF, y_train) #For sklearn no one hot encoding\n",
    "\n",
    "#Send test data through same feature extractor process\n",
    "X_test_feature = feature_extractor.predict(X_test)\n",
    "#Now predict using the trained RF model. \n",
    "prediction_RF = RF_model.predict(X_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.5752212389380531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "print (\"Accuracy = \", metrics.accuracy_score(y_test, prediction_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 9s 562ms/step\n"
     ]
    }
   ],
   "source": [
    "X_for_RF = feature_extractor.predict(df_interno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Classificator--\n",
      "The classificator is: RF\n",
      "0.5644 accuracy with a standard deviation of 0.0435\n",
      "0.1658 precision with a standard deviation of 0.0420\n",
      "0.2236 recall with a standard deviation of 0.0229\n",
      "0.1778 F1 with a standard deviation of 0.0254\n",
      "-----------------------------------------------------------\n",
      "Random Forest, Hold out, Acc =  0.6444444444444445\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ramdomForestClassificator(X_for_RF, classe, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Classificator--\n",
      "The classificator is: GNB\n",
      "0.4556 accuracy with a standard deviation of 0.0531\n",
      "0.2602 precision with a standard deviation of 0.0830\n",
      "0.2282 recall with a standard deviation of 0.0312\n",
      "0.2224 F1 with a standard deviation of 0.0395\n",
      "-----------------------------------------------------------\n",
      "Gaussian Naive Bayes, Hold out, Acc =  0.7044444444444444\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "naivesBayesClassificator(X_for_RF, classe, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Classificator--\n",
      "The classificator is: SVM\n",
      "0.5651 accuracy with a standard deviation of 0.0078\n",
      "0.1302 precision with a standard deviation of 0.0156\n",
      "0.2300 recall with a standard deviation of 0.0245\n",
      "0.1662 F1 with a standard deviation of 0.0191\n",
      "-----------------------------------------------------------\n",
      "SVM, Hold out, Acc =  0.6444444444444445\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "svmclassificator(X_for_RF, classe, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Classificator--\n",
      "The classificator is: MLP\n",
      "0.3074 accuracy with a standard deviation of 0.1951\n",
      "0.1459 precision with a standard deviation of 0.1340\n",
      "0.2473 recall with a standard deviation of 0.0348\n",
      "0.1289 F1 with a standard deviation of 0.0724\n",
      "-----------------------------------------------------------\n",
      "Saída da rede:\t [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 1 4 4 4 4 4 1 4 4 4 4 4 4 4 4 4 3 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 3 4 4 4 4 3 4 4 4 4 4 4 4 4 1 4 4 4 4 4 4 4 4 3 4 4 4 4 4 2 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 4 4 4 4 4 4 4 4 1 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "Saída desejada:\t [4 4 2 2 4 2 4 4 3 5 4 4 3 4 4 4 4 3 2 4 4 4 3 3 4 4 2 4 4 4 4 3 4 4 4 5 4\n",
      " 4 4 4 4 4 3 3 4 3 2 4 4 2 4 4 2 4 4 3 4 3 4 4 4 3 4 4 4 4 2 4 3 4 4 4 4 4\n",
      " 4 2 4 4 3 4 4 3 4 4 3 5 5 4 3 3 4 4 3 4 4 4 4 4 4 1 2 4 4 4 2 4 2 4 5 5 3\n",
      " 2 4 4 4 4 4 3 4 4 4 1 4 2 4 2 5 3 3 4 4 4 4 4 4 3 3 4 4 2 3 3 4 5 4 4 5 4\n",
      " 4 3 4 4 1 2 4 3 4 3 4 4 5 4 4 2 3 4 4 2 4 4 4 3 4 2 3 4 5 4 4 4]\n",
      "-----------------------------------------------------------\n",
      "Score:  0.5833333333333334\n",
      "MLP, hold out:  0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "mlpclassificator(X_for_RF, classe, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m adaBoostClassificator(X_for_RF, classe, \u001b[39m5\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\cezar\\OneDrive\\Documentos\\PDI - CLASSIFICACAO\\Classificacao\\classificadores.py:180\u001b[0m, in \u001b[0;36madaBoostClassificator\u001b[1;34m(X, y, folds)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madaBoostClassificator\u001b[39m(X, y, folds):\n\u001b[0;32m    176\u001b[0m \t\u001b[39m# ----------------------- ADA Boost -------------------------------\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \t\u001b[39m# ad = AdaBoostClassifier(n_estimators=10, algorithm=\"SAMME.R\", random_state=1)  # binary\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \tad \u001b[39m=\u001b[39m AdaBoostClassifier(tree\u001b[39m.\u001b[39mDecisionTreeClassifier(max_depth\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m), n_estimators\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,\n\u001b[0;32m    179\u001b[0m \t                        learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)  \u001b[39m# multiclass\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \tcross_validation(\u001b[39m'\u001b[39;49m\u001b[39mAD\u001b[39;49m\u001b[39m'\u001b[39;49m, ad, X, y, folds)\n\u001b[0;32m    181\u001b[0m \t\u001b[39m# scores = cross_val_score(clf, X, y, cv=5)\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \t\u001b[39m# scores.mean()\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \tX_treino, X_teste, y_treino, y_teste \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cezar\\OneDrive\\Documentos\\PDI - CLASSIFICACAO\\Classificacao\\classificadores.py:23\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(name, classifier, X, y, k)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcross_validation\u001b[39m(name, classifier, X, y, k):\n\u001b[0;32m     21\u001b[0m \t\u001b[39m# cross-validation k-folds\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \tfolds \u001b[39m=\u001b[39m k\n\u001b[1;32m---> 23\u001b[0m \tscoresAccuracy \u001b[39m=\u001b[39m cross_val_score(classifier, X, y, cv\u001b[39m=\u001b[39;49mfolds, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     24\u001b[0m \tscoresPrecision \u001b[39m=\u001b[39m cross_val_score(classifier, X, y, cv\u001b[39m=\u001b[39mfolds, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprecision_macro\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \tscoresRecall \u001b[39m=\u001b[39m cross_val_score(classifier, X, y, cv\u001b[39m=\u001b[39mfolds, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecall_macro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:506\u001b[0m, in \u001b[0;36mAdaBoostClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    501\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAlgorithm must be \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSAMME\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSAMME.R\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    505\u001b[0m \u001b[39m# Fit\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:160\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    156\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[0;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m iboost \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators):\n\u001b[0;32m    159\u001b[0m     \u001b[39m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m     sample_weight, estimator_weight, estimator_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_boost(\n\u001b[0;32m    161\u001b[0m         iboost, X, y, sample_weight, random_state\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Early termination\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:568\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \n\u001b[0;32m    531\u001b[0m \u001b[39mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[39m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAMME.R\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 568\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_boost_real(iboost, X, y, sample_weight, random_state)\n\u001b[0;32m    570\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:577\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[39m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m--> 577\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    579\u001b[0m y_predict_proba \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    581\u001b[0m \u001b[39mif\u001b[39;00m iboost \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    970\u001b[0m         X,\n\u001b[0;32m    971\u001b[0m         y,\n\u001b[0;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cezar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adaBoostClassificator(X_for_RF, classe, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95d754cb0e830cdd8cd9fe4526b6df628ca17c2ac4c527b538a04f2b49fc8337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
